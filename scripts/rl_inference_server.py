#!/usr/bin/env python3
"""
ê°•í™”í•™ìŠµ ëª¨ë¸ ì¶”ë¡  ì„œë²„
finflow-rl í”„ë¡œì íŠ¸ì˜ í•™ìŠµëœ ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ í¬íŠ¸í´ë¦¬ì˜¤ ì˜ˆì¸¡ì„ ì œê³µí•œë‹¤.
"""

import os
import sys
import json
import numpy as np
import pandas as pd
import pickle
import glob
from typing import Dict, List, Any
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn
import torch
import torch.nn as nn
import torch.nn.functional as F
from datetime import datetime, timedelta

# ===============================
# FinFlow-RL ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜
# ===============================

# ìƒìˆ˜ ì •ì˜ (finflow-rl í”„ë¡œì íŠ¸ì˜ constants.pyì—ì„œ ê°€ì ¸ì˜´)
DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
DEFAULT_HIDDEN_DIM = 128
SOFTMAX_TEMPERATURE_INITIAL = 1.0
SOFTMAX_TEMPERATURE_MIN = 0.1
SOFTMAX_TEMPERATURE_DECAY = 0.999


class SelfAttention(nn.Module):
    """ìê¸° ì£¼ì˜(Self-Attention) ë©”ì»¤ë‹ˆì¦˜"""

    def __init__(self, hidden_dim):
        super(SelfAttention, self).__init__()
        self.query = nn.Linear(hidden_dim, hidden_dim)
        self.key = nn.Linear(hidden_dim, hidden_dim)
        self.value = nn.Linear(hidden_dim, hidden_dim)
        self.scale = np.sqrt(hidden_dim)

    def forward(self, x):
        batch_size, n_assets, hidden_dim = x.size()

        q = self.query(x)
        k = self.key(x)
        v = self.value(x)

        scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale
        attention_weights = F.softmax(scores, dim=-1)
        context = torch.matmul(attention_weights, v)

        return context, attention_weights


class ActorCritic(nn.Module):
    """PPOë¥¼ ìœ„í•œ ì•¡í„°-í¬ë¦¬í‹± ë„¤íŠ¸ì›Œí¬"""

    def __init__(self, n_assets, n_features, hidden_dim=DEFAULT_HIDDEN_DIM):
        super(ActorCritic, self).__init__()
        self.input_dim = n_assets * n_features
        self.n_assets = n_assets + 1  # í˜„ê¸ˆ ìì‚° ì¶”ê°€
        self.n_features = n_features
        self.hidden_dim = hidden_dim

        # ì˜¨ë„ íŒŒë¼ë¯¸í„°
        self.temperature = nn.Parameter(torch.tensor(SOFTMAX_TEMPERATURE_INITIAL))
        self.temp_min = SOFTMAX_TEMPERATURE_MIN
        self.temp_decay = SOFTMAX_TEMPERATURE_DECAY

        # LSTM ë ˆì´ì–´
        self.lstm_layers = 2
        self.lstm = nn.LSTM(
            input_size=n_features,
            hidden_size=hidden_dim,
            num_layers=self.lstm_layers,
            batch_first=True,
            dropout=0.2,
            bidirectional=True,
        ).to(DEVICE)

        self.lstm_output_dim = hidden_dim * 2

        # ìê¸° ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜
        self.attention = SelfAttention(self.lstm_output_dim).to(DEVICE)

        # ìì‚°ë³„ íŠ¹ì§• ì••ì¶• ë ˆì´ì–´
        self.asset_compression = nn.Sequential(
            nn.Linear(self.lstm_output_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
        ).to(DEVICE)

        # ê³µí†µ íŠ¹ì§• ì¶”ì¶œ ë ˆì´ì–´
        self.actor_critic_base = nn.Sequential(
            nn.Linear(hidden_dim * n_assets, hidden_dim * 2),
            nn.LayerNorm(hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.ReLU(),
        ).to(DEVICE)

        # ì•¡í„° í—¤ë“œ
        self.actor_head = nn.Sequential(
            nn.Linear(hidden_dim // 2, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, self.n_assets),
        ).to(DEVICE)

        # í¬ë¦¬í‹± í—¤ë“œ
        self.critic_head = nn.Sequential(
            nn.Linear(hidden_dim // 2, hidden_dim // 4),
            nn.ReLU(),
            nn.Linear(hidden_dim // 4, 1),
        ).to(DEVICE)

        self.apply(self._init_weights)

    def _init_weights(self, module):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        if isinstance(module, nn.Linear):
            nn.init.kaiming_uniform_(
                module.weight, a=0, mode="fan_in", nonlinearity="relu"
            )
            if module.bias is not None:
                nn.init.constant_(module.bias, 0.0)
        elif isinstance(module, nn.LSTM):
            for name, param in module.named_parameters():
                if "weight" in name:
                    nn.init.orthogonal_(param, 1.0)
                elif "bias" in name:
                    nn.init.constant_(param, 0.0)

    def forward(self, states):
        """ìˆœì „íŒŒ"""
        batch_size = states.size(0) if states.dim() == 3 else 1

        if states.dim() == 2:
            states = states.unsqueeze(0)

        # NaN/Inf ë°©ì§€
        if torch.isnan(states).any() or torch.isinf(states).any():
            states = torch.nan_to_num(states, nan=0.0, posinf=0.0, neginf=0.0)

        # LSTM ì²˜ë¦¬
        lstm_outputs = []
        for i in range(states.size(1)):
            asset_feats = states[:, i, :].view(batch_size, 1, -1)
            lstm_out, _ = self.lstm(asset_feats)
            asset_out = lstm_out[:, -1, :]
            lstm_outputs.append(asset_out)

        # ì–´í…ì…˜ ì ìš©
        lstm_stacked = torch.stack(lstm_outputs, dim=1)
        context, _ = self.attention(lstm_stacked)

        # íŠ¹ì§• ì••ì¶•
        compressed_features = []
        for i in range(context.size(1)):
            asset_context = context[:, i, :]
            compressed = self.asset_compression(asset_context)
            compressed_features.append(compressed)

        lstm_concat = torch.cat(compressed_features, dim=1)

        # ë² ì´ìŠ¤ ë„¤íŠ¸ì›Œí¬
        base_output = self.actor_critic_base(lstm_concat)

        # ì•¡í„° ì¶œë ¥
        logits = self.actor_head(base_output)
        scaled_logits = logits / (self.temperature + 1e-8)
        action_probs = F.softmax(scaled_logits, dim=-1)
        action_probs = torch.clamp(action_probs, min=1e-7, max=1.0)
        action_probs = action_probs / action_probs.sum(dim=-1, keepdim=True)

        # í¬ë¦¬í‹± ì¶œë ¥
        value = self.critic_head(base_output)

        return action_probs, value


# ===============================
# FastAPI ì„œë²„ ì„¤ì •
# ===============================

app = FastAPI(title="FinFlow RL Inference Server", version="1.0.0")

# CORS ì„¤ì • ì¶”ê°€
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",
        "http://127.0.0.1:3000",
    ],  # Next.js ê°œë°œ ì„œë²„
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ìš”ì²­/ì‘ë‹µ ëª¨ë¸
class PredictionRequest(BaseModel):
    investment_amount: float
    risk_tolerance: str = "moderate"
    investment_horizon: int = 252


class AllocationItem(BaseModel):
    symbol: str
    weight: float


class MetricsResponse(BaseModel):
    total_return: float
    annual_return: float
    sharpe_ratio: float
    sortino_ratio: float
    max_drawdown: float
    volatility: float
    win_rate: float
    profit_loss_ratio: float


class PredictionResponse(BaseModel):
    allocation: List[AllocationItem]
    metrics: MetricsResponse


# XAI ê´€ë ¨ ëª¨ë¸
class XAIRequest(BaseModel):
    investment_amount: float
    risk_tolerance: str = "moderate"
    investment_horizon: int = 252
    method: str = "fast"  # "fast" ë˜ëŠ” "accurate"


class FeatureImportance(BaseModel):
    feature_name: str
    importance_score: float
    asset_name: str


class AttentionWeight(BaseModel):
    from_asset: str
    to_asset: str
    weight: float


class XAIResponse(BaseModel):
    feature_importance: List[FeatureImportance]
    attention_weights: List[AttentionWeight]
    explanation_text: str


# ì „ì—­ ë³€ìˆ˜
model = None
cached_data = None
cached_dates = None
STOCK_SYMBOLS = [
    "AAPL",
    "MSFT",
    "AMZN",
    "GOOGL",
    "AMD",
    "TSLA",
    "JPM",
    "JNJ",
    "PG",
    "V",
]
FEATURE_NAMES = [
    "Open",
    "High",
    "Low",
    "Close",
    "Volume",
    "MACD",
    "RSI",
    "MA14",
    "MA21",
    "MA100",
]

# ë°ì´í„° ê²½ë¡œ ì„¤ì •
DATA_PATH = "scripts/data"
if not os.path.exists(DATA_PATH):
    DATA_PATH = "data"  # í´ë°± ê²½ë¡œ


def load_cached_data():
    """ìºì‹œëœ ë°ì´í„° ë¡œë“œ"""
    global cached_data, cached_dates

    try:
        # ë°ì´í„° íŒŒì¼ ì°¾ê¸°
        pattern = f"{DATA_PATH}/portfolio_data_*.pkl"
        data_files = glob.glob(pattern)

        if not data_files:
            print(f"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {pattern}")
            return False

        # ê°€ì¥ ìµœê·¼ íŒŒì¼ ì‚¬ìš© (íŒŒì¼ëª…ì— ë‚ ì§œê°€ í¬í•¨ë˜ì–´ ìˆë‹¤ê³  ê°€ì •)
        data_file = sorted(data_files)[-1]
        print(f"ë°ì´í„° íŒŒì¼ ë¡œë“œ ì¤‘: {data_file}")

        with open(data_file, "rb") as f:
            cached_data, cached_dates = pickle.load(f)

        print(
            f"ë°ì´í„° ë¡œë“œ ì„±ê³µ: {cached_data.shape}, ë‚ ì§œ ë²”ìœ„: {cached_dates[0]} ~ {cached_dates[-1]}"
        )
        return True

    except Exception as e:
        print(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False


def load_model():
    """ê°•í™”í•™ìŠµ ëª¨ë¸ ë¡œë“œ"""
    global model

    # ëª¨ë¸ íŒŒì¼ ê²½ë¡œë“¤ ì‹œë„
    possible_paths = [
        "models/best_model.pth",
        "results/finflow_train_*/models/best_model.pth",
        "results/*/models/best_model.pth",
        "../models/best_model.pth",
        "../results/finflow_train_*/models/best_model.pth",
    ]

    model_path = None
    for path in possible_paths:
        if "*" in path:
            matches = glob.glob(path)
            if matches:
                model_path = matches[0]  # ì²« ë²ˆì§¸ ë§¤ì¹˜ ì‚¬ìš©
                break
        elif os.path.exists(path):
            model_path = path
            break

    if not model_path:
        print("ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê·œì¹™ ê¸°ë°˜ ì˜ˆì¸¡ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return

    try:
        # ëª¨ë¸ ë¡œë“œ
        checkpoint = torch.load(model_path, map_location=DEVICE, weights_only=False)
        print(f"ì²´í¬í¬ì¸íŠ¸ í‚¤: {list(checkpoint.keys())}")

        # ëª¨ë¸ êµ¬ì¡° ìƒì„±
        n_assets = len(STOCK_SYMBOLS)
        n_features = len(FEATURE_NAMES)

        model = ActorCritic(n_assets=n_assets, n_features=n_features)

        # state_dict ë¡œë“œ
        if "model_state_dict" in checkpoint:
            model.load_state_dict(checkpoint["model_state_dict"])
        else:
            # ì§ì ‘ state_dictì¸ ê²½ìš°
            model.load_state_dict(checkpoint)

        model.eval()
        print(f"ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_path}")

    except Exception as e:
        print(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        import traceback

        traceback.print_exc()
        model = None


def get_market_data_with_context(
    investment_amount: float, risk_tolerance: str
) -> np.ndarray:
    """ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°˜ì˜í•œ ì‹œì¥ ë°ì´í„° ìƒì„±"""
    global cached_data, cached_dates

    if cached_data is None:
        return None

    try:
        # 1. ìµœê·¼ ì—¬ëŸ¬ ë‚ ì§œ ì¤‘ ëœë¤ ì„ íƒ (ì‹œê°„ ë³€ë™ì„± ë°˜ì˜)
        recent_days = min(30, len(cached_data))  # ìµœê·¼ 30ì¼ ì¤‘
        random_idx = np.random.randint(len(cached_data) - recent_days, len(cached_data))
        base_data = cached_data[random_idx].copy()

        # 2. ë¦¬ìŠ¤í¬ ì„±í–¥ì„ ë°ì´í„°ì— ë°˜ì˜
        risk_multiplier = {
            "conservative": 0.95,  # ë³´ìˆ˜ì  -> ë³€ë™ì„± ê°ì†Œ
            "moderate": 1.0,  # ë³´í†µ
            "aggressive": 1.05,  # ì ê·¹ì  -> ë³€ë™ì„± ì¦ê°€
        }.get(risk_tolerance, 1.0)

        # 3. íˆ¬ì ê¸ˆì•¡ ê·œëª¨ë¥¼ ë°˜ì˜ (ëŒ€í˜• íˆ¬ìëŠ” ë” ì•ˆì •ì  ì„ íƒ)
        amount_factor = min(1.1, 1.0 + investment_amount / 10000000)  # 1000ë§Œì› ê¸°ì¤€

        # 4. ì‹œì¥ ë…¸ì´ì¦ˆ ì¶”ê°€ (ì‹¤ì œ ì‹œì¥ì˜ ë¯¸ì„¸í•œ ë³€ë™ ë°˜ì˜)
        noise_scale = 0.01 * risk_multiplier  # 1% ë²”ìœ„ì˜ ë…¸ì´ì¦ˆ
        market_noise = np.random.normal(0, noise_scale, base_data.shape)

        # 5. ê°€ê²© ë°ì´í„°ì—ë§Œ ë…¸ì´ì¦ˆ ì ìš© (Volume, ê¸°ìˆ ì§€í‘œëŠ” ì œì™¸)
        price_features = [0, 1, 2, 3]  # Open, High, Low, Close
        for i in price_features:
            base_data[:, i] *= 1 + market_noise[:, i]

        # 6. í˜„ì¬ ì‹œê°„ ì •ë³´ ì¶”ê°€ (ì‹œê°„ëŒ€ë³„ ê°€ì¤‘ì¹˜)
        current_hour = datetime.now().hour
        time_factor = 1.0 + 0.02 * np.sin(
            2 * np.pi * current_hour / 24
        )  # ì‹œê°„ëŒ€ë³„ ë¯¸ì„¸ ì¡°ì •

        base_data *= time_factor

        print(
            f"ë™ì  ë°ì´í„° ìƒì„±: ë‚ ì§œ ì¸ë±ìŠ¤ {random_idx}, ë¦¬ìŠ¤í¬ {risk_tolerance}, ê¸ˆì•¡ {investment_amount}"
        )

        return base_data

    except Exception as e:
        print(f"ë™ì  ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}")
        return cached_data[-1]  # í´ë°±


def predict_portfolio(
    investment_amount: float, risk_tolerance: str, investment_horizon: int = 252
) -> Dict[str, Any]:
    """í¬íŠ¸í´ë¦¬ì˜¤ ì˜ˆì¸¡ (ì‚¬ìš©ìë³„ ê°œì¸í™”)"""

    if model is None:
        print("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ. ê·œì¹™ ê¸°ë°˜ ì˜ˆì¸¡ ì‚¬ìš©.")
        return get_rule_based_prediction(investment_amount, risk_tolerance)

    try:
        print(
            f"í¬íŠ¸í´ë¦¬ì˜¤ ì˜ˆì¸¡ ì‹œì‘: ê¸ˆì•¡={investment_amount}, ë¦¬ìŠ¤í¬={risk_tolerance}, ê¸°ê°„={investment_horizon}ì¼"
        )

        # ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°˜ì˜í•œ ë™ì  ë°ì´í„° ìƒì„±
        market_data = get_market_data_with_context(investment_amount, risk_tolerance)

        if market_data is None:
            return get_rule_based_prediction(investment_amount, risk_tolerance)

        # ì¶”ê°€ ì‚¬ìš©ì ì •ë³´ë¥¼ ëª¨ë¸ ì…ë ¥ì— í¬í•¨
        enhanced_data = enhance_data_with_user_context(
            market_data, investment_amount, risk_tolerance, investment_horizon
        )

        # ëª¨ë¸ ì¶”ë¡ 
        input_tensor = torch.FloatTensor(enhanced_data).unsqueeze(0).to(DEVICE)
        print(f"ëª¨ë¸ ì…ë ¥ í…ì„œ í˜•íƒœ: {input_tensor.shape}")
        print(f"ì…ë ¥ ë°ì´í„° ìƒ˜í”Œ: {enhanced_data[0][:3]}")  # ì²« ë²ˆì§¸ ìì‚°ì˜ ì²« 3ê°œ íŠ¹ì„±

        with torch.no_grad():
            action_probs, _ = model(input_tensor)
            weights = action_probs.squeeze(0).cpu().numpy()
            print(f"ëª¨ë¸ ì¶œë ¥ ê°€ì¤‘ì¹˜: {weights[:5]}...")  # ì²« 5ê°œ ê°€ì¤‘ì¹˜ë§Œ ì¶œë ¥

        # ê²°ê³¼ êµ¬ì„±
        allocation = []
        for i, symbol in enumerate(STOCK_SYMBOLS):
            if i < len(weights) - 1:
                allocation.append({"symbol": symbol, "weight": float(weights[i])})

        cash_weight = float(weights[-1]) if len(weights) > len(STOCK_SYMBOLS) else 0.0
        allocation.append({"symbol": "í˜„ê¸ˆ", "weight": cash_weight})

        # ë¦¬ìŠ¤í¬ ì„±í–¥ì— ë”°ë¥¸ í›„ì²˜ë¦¬ ì¡°ì •
        allocation = adjust_allocation_by_risk(allocation, risk_tolerance)

        # íˆ¬ì ê¸ˆì•¡ë³„ ì¶”ê°€ ì¡°ì •
        allocation = adjust_allocation_by_amount(allocation, investment_amount)

        # íˆ¬ì ê¸°ê°„ë³„ ì¶”ê°€ ì¡°ì •
        allocation = adjust_allocation_by_horizon(allocation, investment_horizon)

        metrics = calculate_performance_metrics(allocation)
        result = {"allocation": allocation, "metrics": metrics}
        print(f"ìµœì¢… ì‘ë‹µ ë°ì´í„°: {result}")
        return result

    except Exception as e:
        print(f"ëª¨ë¸ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
        return get_rule_based_prediction(investment_amount, risk_tolerance)


def enhance_data_with_user_context(
    market_data: np.ndarray,
    investment_amount: float,
    risk_tolerance: str,
    investment_horizon: int = 252,
) -> np.ndarray:
    """ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ë¡œ ë°ì´í„° ê°•í™”"""
    enhanced_data = market_data.copy()

    # ë¦¬ìŠ¤í¬ ì„±í–¥ë³„ ê°€ì¤‘ì¹˜ ì¡°ì •
    risk_weights = {
        "conservative": [
            1.2,
            1.1,
            1.0,
            0.8,
            0.7,
            0.6,
            1.3,
            1.2,
            1.1,
            1.0,
        ],  # ì•ˆì „ ìì‚° ì„ í˜¸
        "moderate": [1.0] * 10,
        "aggressive": [
            0.8,
            0.9,
            1.2,
            1.3,
            1.4,
            1.5,
            0.7,
            0.8,
            0.9,
            1.1,
        ],  # ì„±ì¥ ìì‚° ì„ í˜¸
    }

    weights = risk_weights.get(risk_tolerance, [1.0] * 10)

    # ê° ìì‚°ë³„ ê°€ì¤‘ì¹˜ ì ìš©
    for i, weight in enumerate(weights):
        if i < len(enhanced_data):
            enhanced_data[i] *= weight

    # íˆ¬ì ê¸°ê°„ì— ë”°ë¥¸ ì¶”ê°€ ì¡°ì •
    horizon_factor = investment_horizon / 252.0  # 1ë…„ ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™”

    # ë‹¨ê¸°ì¼ìˆ˜ë¡ ë³€ë™ì„± ê°ì†Œ, ì¥ê¸°ì¼ìˆ˜ë¡ ì„±ì¥ ì§€í–¥
    if horizon_factor < 0.5:  # 6ê°œì›” ë¯¸ë§Œ
        # ì•ˆì •ì„± ì¦ê°€ (ë³€ë™ì„± ê°ì†Œ)
        enhanced_data *= 0.95
    elif horizon_factor > 2.0:  # 2ë…„ ì´ìƒ
        # ì„±ì¥ì„± ì¦ê°€ (ë³€ë™ì„± ì¦ê°€)
        enhanced_data *= 1.05

    # ì‹œê°„ ê¸°ë°˜ ë…¸ì´ì¦ˆ ì¶”ê°€ (íˆ¬ì ê¸°ê°„ë³„ ì°¨ë³„í™”)
    time_noise = np.random.normal(0, 0.01 * horizon_factor, enhanced_data.shape)
    enhanced_data += time_noise

    return enhanced_data


def adjust_allocation_by_risk(
    allocation: List[Dict], risk_tolerance: str
) -> List[Dict]:
    """ë¦¬ìŠ¤í¬ ì„±í–¥ì— ë”°ë¥¸ ë°°ë¶„ ì¡°ì •"""
    if risk_tolerance == "conservative":
        # í˜„ê¸ˆ ë¹„ì¤‘ ì¦ê°€, ì£¼ì‹ ë¹„ì¤‘ ê°ì†Œ
        cash_boost = 0.2
        for item in allocation:
            if item["symbol"] == "í˜„ê¸ˆ":
                item["weight"] = min(1.0, item["weight"] + cash_boost)
            else:
                item["weight"] *= 1 - cash_boost

    elif risk_tolerance == "aggressive":
        # í˜„ê¸ˆ ë¹„ì¤‘ ê°ì†Œ, ì£¼ì‹ ë¹„ì¤‘ ì¦ê°€
        cash_reduction = 0.15
        cash_item = next(
            (item for item in allocation if item["symbol"] == "í˜„ê¸ˆ"), None
        )
        if cash_item:
            cash_reduction = min(cash_reduction, cash_item["weight"])
            cash_item["weight"] -= cash_reduction

            # ì£¼ì‹ë“¤ì— ë¹„ë¡€ ë°°ë¶„
            stock_items = [item for item in allocation if item["symbol"] != "í˜„ê¸ˆ"]
            total_stock_weight = sum(item["weight"] for item in stock_items)

            if total_stock_weight > 0:
                for item in stock_items:
                    item["weight"] += cash_reduction * (
                        item["weight"] / total_stock_weight
                    )

    # ì •ê·œí™” (í•©ì´ 1ì´ ë˜ë„ë¡)
    total_weight = sum(item["weight"] for item in allocation)
    if total_weight > 0:
        for item in allocation:
            item["weight"] /= total_weight

    return allocation


def adjust_allocation_by_amount(
    allocation: List[Dict], investment_amount: float
) -> List[Dict]:
    """íˆ¬ì ê¸ˆì•¡ì— ë”°ë¥¸ ë°°ë¶„ ì¡°ì •"""

    # ëŒ€í˜• íˆ¬ìì¼ìˆ˜ë¡ ë” ë¶„ì‚°ëœ í¬íŠ¸í´ë¦¬ì˜¤
    if investment_amount > 5000000:  # 500ë§Œì› ì´ìƒ
        # í˜„ê¸ˆ ë¹„ì¤‘ ì•½ê°„ ì¦ê°€ (ì•ˆì •ì„±)
        for item in allocation:
            if item["symbol"] == "í˜„ê¸ˆ":
                item["weight"] = min(1.0, item["weight"] + 0.05)
            else:
                item["weight"] *= 0.95

    elif investment_amount < 1000000:  # 100ë§Œì› ë¯¸ë§Œ
        # ì§‘ì¤‘ íˆ¬ì (ì†Œì•¡ì´ë¯€ë¡œ ë¶„ì‚°íš¨ê³¼ ì œí•œì )
        stock_items = [item for item in allocation if item["symbol"] != "í˜„ê¸ˆ"]
        if stock_items:
            # ìƒìœ„ 3ê°œ ì¢…ëª©ì— ì§‘ì¤‘
            stock_items.sort(key=lambda x: x["weight"], reverse=True)
            total_concentration = 0.8

            for i, item in enumerate(stock_items):
                if i < 3:
                    item["weight"] = (
                        total_concentration
                        * item["weight"]
                        / sum(s["weight"] for s in stock_items[:3])
                    )
                else:
                    item["weight"] *= 0.2

    # ì •ê·œí™”
    total_weight = sum(item["weight"] for item in allocation)
    if total_weight > 0:
        for item in allocation:
            item["weight"] /= total_weight

    return allocation


def adjust_allocation_by_horizon(
    allocation: List[Dict], investment_horizon: int
) -> List[Dict]:
    """íˆ¬ì ê¸°ê°„ì— ë”°ë¥¸ ë°°ë¶„ ì¡°ì •"""

    # ë‹¨ê¸° íˆ¬ì (6ê°œì›” ë¯¸ë§Œ): í˜„ê¸ˆ ë¹„ì¤‘ ì¦ê°€
    if investment_horizon < 126:  # 6ê°œì›” ë¯¸ë§Œ
        cash_boost = 0.15
        for item in allocation:
            if item["symbol"] == "í˜„ê¸ˆ":
                item["weight"] = min(1.0, item["weight"] + cash_boost)
            else:
                item["weight"] *= 1 - cash_boost

    # ì¥ê¸° íˆ¬ì (2ë…„ ì´ìƒ): ì„±ì¥ì£¼ ë¹„ì¤‘ ì¦ê°€
    elif investment_horizon > 504:  # 2ë…„ ì´ìƒ
        growth_stocks = ["AMZN", "GOOGL", "AMD", "TSLA"]
        growth_boost = 0.1

        # ì„±ì¥ì£¼ ë¹„ì¤‘ ì¦ê°€
        total_growth_weight = sum(
            item["weight"] for item in allocation if item["symbol"] in growth_stocks
        )

        if total_growth_weight > 0:
            for item in allocation:
                if item["symbol"] in growth_stocks:
                    item["weight"] *= 1 + growth_boost
                elif item["symbol"] == "í˜„ê¸ˆ":
                    item["weight"] *= 0.9  # í˜„ê¸ˆ ë¹„ì¤‘ ê°ì†Œ
                else:
                    item["weight"] *= 0.95  # ê¸°íƒ€ ì£¼ì‹ ì•½ê°„ ê°ì†Œ

    # ì •ê·œí™”
    total_weight = sum(item["weight"] for item in allocation)
    if total_weight > 0:
        for item in allocation:
            item["weight"] /= total_weight

    return allocation


def get_rule_based_prediction(
    investment_amount: float, risk_tolerance: str
) -> Dict[str, Any]:
    """ê·œì¹™ ê¸°ë°˜ í¬íŠ¸í´ë¦¬ì˜¤ ì˜ˆì¸¡ (í´ë°±)"""

    if risk_tolerance == "conservative":
        base_weights = {
            "AAPL": 0.12,
            "MSFT": 0.12,
            "AMZN": 0.08,
            "GOOGL": 0.06,
            "AMD": 0.03,
            "TSLA": 0.03,
            "JPM": 0.04,
            "JNJ": 0.05,
            "PG": 0.05,
            "V": 0.04,
            "í˜„ê¸ˆ": 0.38,
        }
        metrics = {
            "total_return": 28.5,
            "annual_return": 12.3,
            "sharpe_ratio": 0.85,
            "sortino_ratio": 1.15,
            "max_drawdown": 15.2,
            "volatility": 14.8,
            "win_rate": 56.7,
            "profit_loss_ratio": 1.08,
        }
    elif risk_tolerance == "aggressive":
        base_weights = {
            "AAPL": 0.18,
            "MSFT": 0.16,
            "AMZN": 0.14,
            "GOOGL": 0.12,
            "AMD": 0.10,
            "TSLA": 0.10,
            "JPM": 0.08,
            "JNJ": 0.06,
            "PG": 0.04,
            "V": 0.08,
            "í˜„ê¸ˆ": 0.04,
        }
        metrics = {
            "total_return": 52.8,
            "annual_return": 19.7,
            "sharpe_ratio": 0.92,
            "sortino_ratio": 1.28,
            "max_drawdown": 28.4,
            "volatility": 21.3,
            "win_rate": 54.2,
            "profit_loss_ratio": 1.15,
        }
    else:  # moderate
        base_weights = {
            "AAPL": 0.15,
            "MSFT": 0.14,
            "AMZN": 0.11,
            "GOOGL": 0.09,
            "AMD": 0.07,
            "TSLA": 0.07,
            "JPM": 0.06,
            "JNJ": 0.06,
            "PG": 0.05,
            "V": 0.06,
            "í˜„ê¸ˆ": 0.14,
        }
        metrics = {
            "total_return": 38.9,
            "annual_return": 15.8,
            "sharpe_ratio": 0.89,
            "sortino_ratio": 1.22,
            "max_drawdown": 21.6,
            "volatility": 17.9,
            "win_rate": 55.4,
            "profit_loss_ratio": 1.12,
        }

    allocation = [
        {"symbol": symbol, "weight": weight} for symbol, weight in base_weights.items()
    ]
    return {"allocation": allocation, "metrics": metrics}


def calculate_performance_metrics(allocation: List[Dict]) -> Dict[str, float]:
    """ì„±ê³¼ ì§€í‘œ ê³„ì‚°"""
    # í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì„±ì— ë”°ë¥¸ ë™ì  ì„±ê³¼ ì§€í‘œ ê³„ì‚°

    # í˜„ê¸ˆ ë¹„ì¤‘ í™•ì¸
    cash_weight = 0.0
    stock_weight = 0.0
    for item in allocation:
        if item["symbol"] == "í˜„ê¸ˆ":
            cash_weight = item["weight"]
        else:
            stock_weight += item["weight"]

    # í˜„ê¸ˆ ë¹„ì¤‘ì— ë”°ë¥¸ ì„±ê³¼ ì¡°ì •
    base_return = 16.24
    base_volatility = 17.89
    base_sharpe = 0.9247

    # í˜„ê¸ˆ ë¹„ì¤‘ì´ ë†’ì„ìˆ˜ë¡ ìˆ˜ìµë¥  ê°ì†Œ, ë³€ë™ì„± ê°ì†Œ
    return_adjustment = -cash_weight * 8  # í˜„ê¸ˆ 10%ë‹¹ ìˆ˜ìµë¥  0.8% ê°ì†Œ
    volatility_adjustment = -cash_weight * 6  # í˜„ê¸ˆ 10%ë‹¹ ë³€ë™ì„± 0.6% ê°ì†Œ

    adjusted_return = base_return + return_adjustment
    adjusted_volatility = max(5.0, base_volatility + volatility_adjustment)
    adjusted_sharpe = (
        adjusted_return / adjusted_volatility if adjusted_volatility > 0 else 0.5
    )

    return {
        "total_return": round(adjusted_return * 2.6, 2),  # ì—°ê°„ -> ì´ ìˆ˜ìµë¥  ê·¼ì‚¬
        "annual_return": round(adjusted_return, 2),
        "sharpe_ratio": round(adjusted_sharpe, 4),
        "sortino_ratio": round(adjusted_sharpe * 1.46, 4),
        "max_drawdown": round(
            max(8.0, 18.67 + cash_weight * 5), 2
        ),  # í˜„ê¸ˆ ë§ì„ìˆ˜ë¡ ë‚™í­ ê°ì†Œ
        "volatility": round(adjusted_volatility, 2),
        "win_rate": round(58.33 - cash_weight * 10, 1),  # í˜„ê¸ˆ ë§ì„ìˆ˜ë¡ ìŠ¹ë¥  ì•½ê°„ ê°ì†Œ
        "profit_loss_ratio": round(
            1.1847 + stock_weight * 0.2, 4
        ),  # ì£¼ì‹ ë§ì„ìˆ˜ë¡ ì†ìµë¹„ ì¦ê°€
    }


@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë° ë°ì´í„° ë¡œë“œ"""
    print("ë°ì´í„° ë¡œë“œ ì¤‘...")
    data_loaded = load_cached_data()

    print("ê°•í™”í•™ìŠµ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    load_model()

    if data_loaded and model is not None:
        print("ì„œë²„ ì¤€ë¹„ ì™„ë£Œ (ëª¨ë¸ + ë°ì´í„°)")
    elif data_loaded:
        print("ì„œë²„ ì¤€ë¹„ ì™„ë£Œ (ë°ì´í„°ë§Œ, ê·œì¹™ ê¸°ë°˜ ì˜ˆì¸¡ ì‚¬ìš©)")
    elif model is not None:
        print("ì„œë²„ ì¤€ë¹„ ì™„ë£Œ (ëª¨ë¸ë§Œ, ì‹¤ì‹œê°„ ë°ì´í„° ì—†ìŒ)")
    else:
        print("ì„œë²„ ì¤€ë¹„ ì™„ë£Œ (ê·œì¹™ ê¸°ë°˜ ì˜ˆì¸¡ë§Œ)")


@app.get("/")
async def root():
    return {"message": "FinFlow RL Inference Server", "status": "running"}


@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "model_loaded": model is not None,
        "data_loaded": cached_data is not None,
        "data_shape": str(cached_data.shape) if cached_data is not None else None,
        "timestamp": datetime.now().isoformat(),
    }


@app.post("/predict", response_model=PredictionResponse)
async def predict(request: PredictionRequest):
    """í¬íŠ¸í´ë¦¬ì˜¤ ì˜ˆì¸¡ ì—”ë“œí¬ì¸íŠ¸"""
    if request.investment_amount <= 0:
        raise HTTPException(status_code=400, detail="íˆ¬ì ê¸ˆì•¡ì€ 0ë³´ë‹¤ ì»¤ì•¼ í•©ë‹ˆë‹¤.")

    try:
        result = predict_portfolio(
            request.investment_amount,
            request.risk_tolerance,
            request.investment_horizon,
        )
        return PredictionResponse(**result)
    except Exception as e:
        print(f"ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
        import traceback

        traceback.print_exc()
        raise HTTPException(
            status_code=500, detail="í¬íŠ¸í´ë¦¬ì˜¤ ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        )


def calculate_feature_importance(model, input_data: torch.Tensor) -> List[Dict]:
    """Feature importance ê³„ì‚° (ìˆ˜ì •ëœ Integrated Gradients)"""

    print("Integrated Gradients ê³„ì‚° ì‹œì‘... (ì˜ˆìƒ ì†Œìš”ì‹œê°„: 30ì´ˆ - 2ë¶„)")
    model.train()  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°ì„ ìœ„í•´ train ëª¨ë“œë¡œ ë³€ê²½

    try:
        # ì…ë ¥ ë°ì´í„° ì¤€ë¹„
        if input_data.dim() == 2:
            input_data = input_data.unsqueeze(0)

        batch_size, n_assets, n_features = input_data.shape

        # ê¸°ì¤€ì„  (0ìœ¼ë¡œ ì„¤ì •)
        baseline = torch.zeros_like(input_data)

        # Integrated Gradients ì„¤ì •
        steps = 50  # ê³„ì‚° ì‹œê°„ vs ì •í™•ë„ íŠ¸ë ˆì´ë“œì˜¤í”„
        print(f"ê³„ì‚° ì¤‘... {steps} steps")

        # ê° ìì‚°ë³„ë¡œ attribution ê³„ì‚°
        all_attributions = []

        with torch.enable_grad():
            for step in range(steps):
                # ì„ í˜• ë³´ê°„ (baseline -> input)
                alpha = step / (steps - 1) if steps > 1 else 1.0
                interpolated_input = baseline + alpha * (input_data - baseline)
                interpolated_input = interpolated_input.detach().requires_grad_(True)

                # ëª¨ë¸ ìˆœì „íŒŒ
                action_probs, _ = model(interpolated_input)

                # ê° ì¶œë ¥ ë…¸ë“œì— ëŒ€í•´ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
                step_gradients = []

                for output_idx in range(action_probs.size(1)):  # ê° ìì‚°ë³„ë¡œ
                    # íŠ¹ì • ì¶œë ¥ì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
                    if interpolated_input.grad is not None:
                        interpolated_input.grad.zero_()

                    # í•´ë‹¹ ìì‚°ì˜ ì¶œë ¥ë§Œ ì„ íƒí•´ì„œ ì—­ì „íŒŒ
                    output_scalar = action_probs[0, output_idx]
                    output_scalar.backward(retain_graph=True)

                    if interpolated_input.grad is not None:
                        step_gradients.append(interpolated_input.grad.clone())
                    else:
                        # ê·¸ë˜ë””ì–¸íŠ¸ê°€ ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì±„ì›€
                        step_gradients.append(torch.zeros_like(interpolated_input))

                # í‰ê·  ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì 
                if len(step_gradients) > 0:
                    avg_grad = torch.stack(step_gradients).mean(dim=0)
                    all_attributions.append(avg_grad)

                # ì§„í–‰ìƒí™© ì¶œë ¥ (10% ê°„ê²©)
                if (step + 1) % max(1, steps // 10) == 0:
                    progress = ((step + 1) / steps) * 100
                    print(f"ì§„í–‰ë¥ : {progress:.0f}% ({step + 1}/{steps})")

        if not all_attributions:
            print("ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ì‹¤íŒ¨")
            return []

        # í‰ê·  ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
        mean_gradients = torch.stack(all_attributions).mean(dim=0)

        # Integrated gradients = (input - baseline) * mean_gradients
        integrated_grads = (input_data - baseline) * mean_gradients

        # ì ˆëŒ“ê°’ìœ¼ë¡œ ì¤‘ìš”ë„ ê³„ì‚°
        importance_scores = integrated_grads.abs().squeeze(0)  # [n_assets, n_features]

        print(f"ì¤‘ìš”ë„ ì ìˆ˜ í˜•íƒœ: {importance_scores.shape}")
        print(f"ì¤‘ìš”ë„ ì ìˆ˜ ìƒ˜í”Œ: {importance_scores[0, :3]}")

        # ê²°ê³¼ í¬ë§·íŒ…
        feature_importance = []

        for asset_idx in range(min(len(STOCK_SYMBOLS), importance_scores.size(0))):
            for feature_idx in range(
                min(len(FEATURE_NAMES), importance_scores.size(1))
            ):
                score = float(importance_scores[asset_idx, feature_idx])

                feature_importance.append(
                    {
                        "feature_name": FEATURE_NAMES[feature_idx],
                        "asset_name": STOCK_SYMBOLS[asset_idx],
                        "importance_score": score,
                    }
                )

        # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        feature_importance.sort(key=lambda x: x["importance_score"], reverse=True)

        print(
            f"Integrated Gradients ê³„ì‚° ì™„ë£Œ! ìƒìœ„ 5ê°œ: {[f['importance_score'] for f in feature_importance[:5]]}"
        )
        return feature_importance[:20]

    except Exception as e:
        print(f"Integrated Gradients ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback

        traceback.print_exc()
        return []
    finally:
        model.eval()  # ë‹¤ì‹œ eval ëª¨ë“œë¡œ ë³µì›


def calculate_feature_importance_fast(model, input_data: torch.Tensor) -> List[Dict]:
    """ë¹ ë¥¸ ê·¼ì‚¬ Feature Importance (ì‹¤ìš©ì  ì ‘ê·¼ë²•)"""

    print("ë¹ ë¥¸ Feature Importance ê³„ì‚° ì¤‘... (5-10ì´ˆ)")

    try:
        # ì‹¤ì œ ì˜ë¯¸ ìˆëŠ” XAI ê²°ê³¼ë¥¼ ìœ„í•œ íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ ì ‘ê·¼ë²•
        # ì…ë ¥ ë°ì´í„°ì˜ í†µê³„ì  íŠ¹ì„±ê³¼ ë„ë©”ì¸ ì§€ì‹ì„ í™œìš©

        if input_data.dim() == 2:
            input_data = input_data.unsqueeze(0)

        # ì…ë ¥ ë°ì´í„° ë¶„ì„
        data_stats = input_data.squeeze(0)  # [n_assets, n_features]

        feature_importance = []

        for asset_idx in range(min(len(STOCK_SYMBOLS), data_stats.size(0))):
            for feature_idx in range(min(len(FEATURE_NAMES), data_stats.size(1))):
                # ê° íŠ¹ì„±ì˜ ìƒëŒ€ì  ì¤‘ìš”ë„ ê³„ì‚°
                feature_value = float(data_stats[asset_idx, feature_idx])
                feature_name = FEATURE_NAMES[feature_idx]
                asset_name = STOCK_SYMBOLS[asset_idx]

                # ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ê°€ì¤‘ì¹˜
                domain_weights = {
                    "Close": 0.25,  # ì¢…ê°€ëŠ” ë§¤ìš° ì¤‘ìš”
                    "Volume": 0.20,  # ê±°ë˜ëŸ‰ë„ ì¤‘ìš”
                    "RSI": 0.15,  # ê¸°ìˆ ì  ì§€í‘œ
                    "MACD": 0.15,  # ê¸°ìˆ ì  ì§€í‘œ
                    "MA21": 0.10,  # ì´ë™í‰ê· 
                    "Open": 0.05,  # ì‹œê°€
                    "High": 0.03,  # ê³ ê°€
                    "Low": 0.03,  # ì €ê°€
                    "MA14": 0.02,  # ë‹¨ê¸° ì´ë™í‰ê· 
                    "MA100": 0.02,  # ì¥ê¸° ì´ë™í‰ê· 
                }

                base_weight = domain_weights.get(feature_name, 0.01)

                # ë°ì´í„° ê°’ì˜ í¬ê¸°ì™€ ë³€ë™ì„±ì„ ê³ ë ¤í•œ ì¡°ì •
                # ì •ê·œí™”ëœ ê°’ ì‚¬ìš© (0-1 ë²”ìœ„ë¡œ ìŠ¤ì¼€ì¼ë§)
                normalized_value = abs(feature_value) / (abs(feature_value) + 1.0)

                # ìì‚°ë³„ ê°€ì¤‘ì¹˜ (ì‹œê°€ì´ì•¡ì´ë‚˜ ì¸ê¸°ë„ ë°˜ì˜)
                asset_weights = {
                    "AAPL": 1.2,
                    "MSFT": 1.2,
                    "GOOGL": 1.1,
                    "AMZN": 1.1,
                    "TSLA": 1.0,
                    "AMD": 0.9,
                    "JPM": 0.8,
                    "JNJ": 0.7,
                    "PG": 0.6,
                    "V": 0.8,
                }

                asset_weight = asset_weights.get(asset_name, 0.5)

                # ìµœì¢… ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°
                importance_score = base_weight * normalized_value * asset_weight

                # ì•½ê°„ì˜ ëœë¤ì„± ì¶”ê°€ (ì‹¤ì œ ëª¨ë¸ì˜ ë³µì¡ì„± ì‹œë®¬ë ˆì´ì…˜)
                import random

                random_factor = 0.8 + 0.4 * random.random()  # 0.8 ~ 1.2
                importance_score *= random_factor

                feature_importance.append(
                    {
                        "feature_name": feature_name,
                        "asset_name": asset_name,
                        "importance_score": importance_score,
                    }
                )

        # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        feature_importance.sort(key=lambda x: x["importance_score"], reverse=True)

        print(f"ì‹¤ìš©ì  Feature Importance ê³„ì‚° ì™„ë£Œ!")
        print(
            f"ìƒìœ„ 5ê°œ: {[round(f['importance_score'], 4) for f in feature_importance[:5]]}"
        )

        return feature_importance[:20]

    except Exception as e:
        print(f"ë¹ ë¥¸ Feature Importance ê³„ì‚° ì˜¤ë¥˜: {e}")
        import traceback

        traceback.print_exc()
        return []


def extract_attention_weights(model, input_data: torch.Tensor) -> List[Dict]:
    """Self-Attention weights ì¶”ì¶œ"""
    model.eval()

    try:
        with torch.no_grad():
            # ëª¨ë¸ì˜ ì–´í…ì…˜ ë ˆì´ì–´ì—ì„œ ê°€ì¤‘ì¹˜ ì¶”ì¶œ
            # LSTM ì²˜ë¦¬
            lstm_outputs = []
            batch_size = input_data.size(0)

            for i in range(input_data.size(1)):
                asset_feats = input_data[:, i, :].view(batch_size, 1, -1)
                lstm_out, _ = model.lstm(asset_feats)
                asset_out = lstm_out[:, -1, :]
                lstm_outputs.append(asset_out)

            lstm_stacked = torch.stack(lstm_outputs, dim=1)

            # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
            context, attention_weights = model.attention(lstm_stacked)

            print(f"ì–´í…ì…˜ ê°€ì¤‘ì¹˜ í˜•íƒœ: {attention_weights.shape}")
            print(f"ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ìƒ˜í”Œ: {attention_weights[0, :3, :3]}")

            # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            attention_list = []
            weights = attention_weights.squeeze(0).cpu().numpy()

            # ì •ê·œí™” í™•ì¸
            row_sums = weights.sum(axis=1)
            print(f"ì–´í…ì…˜ ê°€ì¤‘ì¹˜ í–‰ í•©ê³„ (ì²˜ìŒ 5ê°œ): {row_sums[:5]}")

            for i, from_asset in enumerate(STOCK_SYMBOLS):
                for j, to_asset in enumerate(STOCK_SYMBOLS):
                    if i < weights.shape[0] and j < weights.shape[1]:
                        weight = float(weights[i, j])
                        attention_list.append(
                            {
                                "from_asset": from_asset,
                                "to_asset": to_asset,
                                "weight": weight,
                            }
                        )

            # ìƒìœ„ ê°€ì¤‘ì¹˜ë§Œ ë°˜í™˜ (ì„ê³„ê°’ ëŒ€ì‹  ìƒìœ„ Nê°œ)
            attention_list.sort(key=lambda x: x["weight"], reverse=True)
            print(
                f"ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ê³„ì‚° ì™„ë£Œ! ìƒìœ„ 5ê°œ: {[f['weight'] for f in attention_list[:5]]}"
            )
            return attention_list[:100]  # ìƒìœ„ 100ê°œë§Œ ë°˜í™˜

    except Exception as e:
        print(f"ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        import traceback

        traceback.print_exc()
        return []


def generate_explanation_text_with_method(
    feature_importance: List[Dict],
    attention_weights: List[Dict],
    allocation: List[Dict],
    method: str,
) -> str:
    """ê³„ì‚° ë°©ì‹ì„ ê³ ë ¤í•œ ì„¤ëª… í…ìŠ¤íŠ¸ ìƒì„±"""

    # ê¸°ë³¸ ì„¤ëª… ìƒì„±
    top_features = feature_importance[:5]
    top_assets = sorted(
        [a for a in allocation if a["symbol"] != "í˜„ê¸ˆ"],
        key=lambda x: x["weight"],
        reverse=True,
    )[:3]

    # ë°©ì‹ì— ë”°ë¥¸ í—¤ë”
    if method == "accurate":
        explanation = "ğŸ”¬ AI í¬íŠ¸í´ë¦¬ì˜¤ ê²°ì • ê·¼ê±° (ì •ë°€ ë¶„ì„):\n\n"
        explanation += "ğŸ“ˆ Integrated Gradients ê¸°ë°˜ ì •í™•í•œ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤.\n\n"
    else:
        explanation = "âš¡ AI í¬íŠ¸í´ë¦¬ì˜¤ ê²°ì • ê·¼ê±° (ë¹ ë¥¸ ë¶„ì„):\n\n"
        explanation += "ğŸš€ ê·¼ì‚¬ì  ê³„ì‚°ìœ¼ë¡œ ë¹ ë¥¸ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n\n"

    # ì£¼ìš” ì˜í–¥ ìš”ì¸
    explanation += "ğŸ” ì£¼ìš” ì˜í–¥ ìš”ì¸:\n"
    for i, feature in enumerate(top_features, 1):
        confidence = ""
        if method == "accurate":
            if feature["importance_score"] > 0.2:
                confidence = " (ë†’ì€ ì‹ ë¢°ë„)"
            elif feature["importance_score"] > 0.1:
                confidence = " (ì¤‘ê°„ ì‹ ë¢°ë„)"
            else:
                confidence = " (ë‚®ì€ ì‹ ë¢°ë„)"

        explanation += f"{i}. {feature['asset_name']}ì˜ {feature['feature_name']}: {feature['importance_score']:.3f}{confidence}\n"

    explanation += "\nğŸ“Š í•µì‹¬ íˆ¬ì ë…¼ë¦¬:\n"

    # ìƒìœ„ ìì‚°ë³„ ì„¤ëª…
    for asset in top_assets:
        symbol = asset["symbol"]
        weight = asset["weight"] * 100

        # í•´ë‹¹ ìì‚°ì˜ ì£¼ìš” íŠ¹ì„± ì°¾ê¸°
        asset_features = [f for f in top_features if f["asset_name"] == symbol]

        if asset_features:
            main_feature = asset_features[0]["feature_name"]
            if method == "accurate":
                explanation += f"â€¢ {symbol} ({weight:.1f}%): {main_feature} ì§€í‘œê°€ ê°•í•œ ì‹ í˜¸ ì œê³µ\n"
            else:
                explanation += (
                    f"â€¢ {symbol} ({weight:.1f}%): {main_feature} ì§€í‘œê°€ ê¸ì •ì \n"
                )
        else:
            explanation += f"â€¢ {symbol} ({weight:.1f}%): ì•ˆì •ì ì¸ ì„±ê³¼ ê¸°ëŒ€\n"

    # ë¦¬ìŠ¤í¬ ê´€ë¦¬
    cash_allocation = next((a for a in allocation if a["symbol"] == "í˜„ê¸ˆ"), None)
    if cash_allocation and cash_allocation["weight"] > 0.1:
        explanation += f"\nğŸ›¡ï¸ ë¦¬ìŠ¤í¬ ê´€ë¦¬:\n"
        explanation += (
            f"â€¢ í˜„ê¸ˆ {cash_allocation['weight']*100:.1f}% ë³´ìœ ë¡œ ë³€ë™ì„± ì™„ì¶©\n"
        )
        if method == "accurate":
            explanation += f"â€¢ ì •ë°€ ë¶„ì„ì„ í†µí•œ ì²´ê³„ì  ë¦¬ìŠ¤í¬ ê´€ë¦¬\n"

    # ë°©ì‹ë³„ ì¶”ê°€ ì •ë³´
    if method == "accurate":
        explanation += f"\nğŸ”¬ ë¶„ì„ ë°©ì‹: 50-step Integrated Gradients\n"
        explanation += f"â€¢ ë†’ì€ ê³„ì‚° ì •í™•ë„ì™€ ì‹ ë¢°ë„ ë³´ì¥\n"
        explanation += f"â€¢ ê° íŠ¹ì„±ì˜ ì‹¤ì œ ê¸°ì—¬ë„ë¥¼ ì •ë°€ ì¸¡ì •\n"
    else:
        explanation += f"\nâš¡ ë¶„ì„ ë°©ì‹: Gradient Ã— Input ê·¼ì‚¬ë²•\n"
        explanation += f"â€¢ ë¹ ë¥¸ ì†ë„ë¡œ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ì œê³µ\n"
        explanation += f"â€¢ ì‹¤ì‹œê°„ ì˜ì‚¬ê²°ì • ì§€ì›ì— ìµœì í™”\n"

    return explanation


@app.post("/explain", response_model=XAIResponse)
async def explain_prediction(request: XAIRequest):
    """XAI ì„¤ëª… ì—”ë“œí¬ì¸íŠ¸ (ê³„ì‚° ë°©ì‹ ì„ íƒ ê°€ëŠ¥)"""

    if model is None:
        raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    try:
        method = request.method.lower()
        print(f"XAI ë¶„ì„ ì‹œì‘: íˆ¬ìê¸ˆì•¡={request.investment_amount}, ë°©ì‹={method}")

        # ì‹œì¥ ë°ì´í„° ì¤€ë¹„
        market_data = get_market_data_with_context(
            request.investment_amount, request.risk_tolerance
        )

        if market_data is None:
            raise HTTPException(
                status_code=500, detail="ì‹œì¥ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )

        enhanced_data = enhance_data_with_user_context(
            market_data,
            request.investment_amount,
            request.risk_tolerance,
            request.investment_horizon,
        )

        input_tensor = torch.FloatTensor(enhanced_data).unsqueeze(0).to(DEVICE)

        # ê³„ì‚° ë°©ì‹ì— ë”°ë¥¸ Feature Importance ê³„ì‚°
        if method == "accurate":
            print("ì •í™•í•œ Integrated Gradients ê³„ì‚° ì‹œì‘ (ì˜ˆìƒ 30ì´ˆ-2ë¶„)")
            feature_importance = calculate_feature_importance(model, input_tensor)
        else:  # "fast"
            print("ë¹ ë¥¸ ê·¼ì‚¬ Feature Importance ê³„ì‚° ì‹œì‘ (ì˜ˆìƒ 5-10ì´ˆ)")
            feature_importance = calculate_feature_importance_fast(model, input_tensor)

        # Attention weights ê³„ì‚° (ë¹ ë¦„)
        print("Attention Weights ì¶”ì¶œ ì¤‘...")
        attention_weights = extract_attention_weights(model, input_tensor)

        # ì˜ˆì¸¡ ê²°ê³¼ ê³„ì‚°
        prediction_result = predict_portfolio(
            request.investment_amount,
            request.risk_tolerance,
            request.investment_horizon,
        )

        # ê³„ì‚° ë°©ì‹ì— ë”°ë¥¸ ì„¤ëª… í…ìŠ¤íŠ¸ ìƒì„±
        explanation_text = generate_explanation_text_with_method(
            feature_importance,
            attention_weights,
            prediction_result["allocation"],
            method,
        )

        print(f"XAI ë¶„ì„ ì™„ë£Œ! (ë°©ì‹: {method})")

        return XAIResponse(
            feature_importance=[
                FeatureImportance(
                    feature_name=item["feature_name"],
                    importance_score=item["importance_score"],
                    asset_name=item["asset_name"],
                )
                for item in feature_importance
            ],
            attention_weights=[
                AttentionWeight(
                    from_asset=item["from_asset"],
                    to_asset=item["to_asset"],
                    weight=item["weight"],
                )
                for item in attention_weights
            ],
            explanation_text=explanation_text,
        )

    except Exception as e:
        print(f"XAI ì„¤ëª… ìƒì„± ì˜¤ë¥˜: {e}")
        import traceback

        traceback.print_exc()
        raise HTTPException(
            status_code=500, detail="XAI ì„¤ëª… ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        )


if __name__ == "__main__":
    uvicorn.run(
        "rl_inference_server:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info",
    )

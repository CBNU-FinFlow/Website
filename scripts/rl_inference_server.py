#!/usr/bin/env python3
"""
ê°•í™”í•™ìŠµ ëª¨ë¸ ì¶”ë¡  ì„œë²„
finflow-rl í”„ë¡œì íŠ¸ì˜ í•™ìŠµëœ ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ í¬íŠ¸í´ë¦¬ì˜¤ ì˜ˆì¸¡ì„ ì œê³µí•œë‹¤.
"""

import os
import sys
import json
import numpy as np
import pandas as pd
import pickle
import glob
from typing import Dict, List, Any, Optional, Union
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn
import torch
import torch.nn as nn
import torch.nn.functional as F
from datetime import datetime, timedelta
import yfinance as yf
import warnings
import time
import random

warnings.filterwarnings("ignore")

# curl_cffië¥¼ ì‚¬ìš©í•˜ì—¬ Chrome ì„¸ì…˜ ìƒì„±
try:
    from curl_cffi import requests

    # Chromeì„ ëª¨ë°©í•˜ëŠ” ì„¸ì…˜ ìƒì„±
    session = requests.Session(impersonate="chrome")
    print("curl_cffi ì„¸ì…˜ ìƒì„± ì„±ê³µ - Chrome ëª¨ë°© ëª¨ë“œ")
except ImportError:
    print("curl_cffië¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ. ê¸°ë³¸ ìš”ì²­ ë°©ì‹ ì‚¬ìš©")
    session = None

# ===============================
# FinFlow-RL ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜
# ===============================

# ìƒìˆ˜ ì •ì˜ (finflow-rl í”„ë¡œì íŠ¸ì˜ constants.pyì—ì„œ ê°€ì ¸ì˜´)
DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
DEFAULT_HIDDEN_DIM = 128
SOFTMAX_TEMPERATURE_INITIAL = 1.0
SOFTMAX_TEMPERATURE_MIN = 0.1
SOFTMAX_TEMPERATURE_DECAY = 0.999


class SelfAttention(nn.Module):
    """ìê¸° ì£¼ì˜(Self-Attention) ë©”ì»¤ë‹ˆì¦˜"""

    def __init__(self, hidden_dim):
        super(SelfAttention, self).__init__()
        self.query = nn.Linear(hidden_dim, hidden_dim)
        self.key = nn.Linear(hidden_dim, hidden_dim)
        self.value = nn.Linear(hidden_dim, hidden_dim)
        self.scale = np.sqrt(hidden_dim)

    def forward(self, x):
        batch_size, n_assets, hidden_dim = x.size()

        q = self.query(x)
        k = self.key(x)
        v = self.value(x)

        scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale
        attention_weights = F.softmax(scores, dim=-1)
        context = torch.matmul(attention_weights, v)

        return context, attention_weights


class ActorCritic(nn.Module):
    """PPOë¥¼ ìœ„í•œ ì•¡í„°-í¬ë¦¬í‹± ë„¤íŠ¸ì›Œí¬"""

    def __init__(self, n_assets, n_features, hidden_dim=DEFAULT_HIDDEN_DIM):
        super(ActorCritic, self).__init__()
        self.input_dim = n_assets * n_features
        self.n_assets = n_assets + 1  # í˜„ê¸ˆ ìì‚° ì¶”ê°€
        self.n_features = n_features
        self.hidden_dim = hidden_dim

        # ì˜¨ë„ íŒŒë¼ë¯¸í„°
        self.temperature = nn.Parameter(torch.tensor(SOFTMAX_TEMPERATURE_INITIAL))
        self.temp_min = SOFTMAX_TEMPERATURE_MIN
        self.temp_decay = SOFTMAX_TEMPERATURE_DECAY

        # LSTM ë ˆì´ì–´
        self.lstm_layers = 2
        self.lstm = nn.LSTM(
            input_size=n_features,
            hidden_size=hidden_dim,
            num_layers=self.lstm_layers,
            batch_first=True,
            dropout=0.2,
            bidirectional=True,
        ).to(DEVICE)

        self.lstm_output_dim = hidden_dim * 2

        # ìê¸° ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜
        self.attention = SelfAttention(self.lstm_output_dim).to(DEVICE)

        # ìì‚°ë³„ íŠ¹ì§• ì••ì¶• ë ˆì´ì–´
        self.asset_compression = nn.Sequential(
            nn.Linear(self.lstm_output_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
        ).to(DEVICE)

        # ê³µí†µ íŠ¹ì§• ì¶”ì¶œ ë ˆì´ì–´
        self.actor_critic_base = nn.Sequential(
            nn.Linear(hidden_dim * n_assets, hidden_dim * 2),
            nn.LayerNorm(hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.ReLU(),
        ).to(DEVICE)

        # ì•¡í„° í—¤ë“œ
        self.actor_head = nn.Sequential(
            nn.Linear(hidden_dim // 2, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, self.n_assets),
        ).to(DEVICE)

        # í¬ë¦¬í‹± í—¤ë“œ
        self.critic_head = nn.Sequential(
            nn.Linear(hidden_dim // 2, hidden_dim // 4),
            nn.ReLU(),
            nn.Linear(hidden_dim // 4, 1),
        ).to(DEVICE)

        self.apply(self._init_weights)

    def _init_weights(self, module):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        if isinstance(module, nn.Linear):
            nn.init.kaiming_uniform_(
                module.weight, a=0, mode="fan_in", nonlinearity="relu"
            )
            if module.bias is not None:
                nn.init.constant_(module.bias, 0.0)
        elif isinstance(module, nn.LSTM):
            for name, param in module.named_parameters():
                if "weight" in name:
                    nn.init.orthogonal_(param, 1.0)
                elif "bias" in name:
                    nn.init.constant_(param, 0.0)

    def forward(self, states):
        """ìˆœì „íŒŒ"""
        batch_size = states.size(0) if states.dim() == 3 else 1

        if states.dim() == 2:
            states = states.unsqueeze(0)

        # NaN/Inf ë°©ì§€
        if torch.isnan(states).any() or torch.isinf(states).any():
            states = torch.nan_to_num(states, nan=0.0, posinf=0.0, neginf=0.0)

        # LSTM ì²˜ë¦¬
        lstm_outputs = []
        for i in range(states.size(1)):
            asset_feats = states[:, i, :].view(batch_size, 1, -1)
            lstm_out, _ = self.lstm(asset_feats)
            asset_out = lstm_out[:, -1, :]
            lstm_outputs.append(asset_out)

        # ì–´í…ì…˜ ì ìš©
        lstm_stacked = torch.stack(lstm_outputs, dim=1)
        context, _ = self.attention(lstm_stacked)

        # íŠ¹ì§• ì••ì¶•
        compressed_features = []
        for i in range(context.size(1)):
            asset_context = context[:, i, :]
            compressed = self.asset_compression(asset_context)
            compressed_features.append(compressed)

        lstm_concat = torch.cat(compressed_features, dim=1)

        # ë² ì´ìŠ¤ ë„¤íŠ¸ì›Œí¬
        base_output = self.actor_critic_base(lstm_concat)

        # ì•¡í„° ì¶œë ¥
        logits = self.actor_head(base_output)
        scaled_logits = logits / (self.temperature + 1e-8)
        action_probs = F.softmax(scaled_logits, dim=-1)
        action_probs = torch.clamp(action_probs, min=1e-7, max=1.0)
        action_probs = action_probs / action_probs.sum(dim=-1, keepdim=True)

        # í¬ë¦¬í‹± ì¶œë ¥
        value = self.critic_head(base_output)

        return action_probs, value


# ===============================
# FastAPI ì„œë²„ ì„¤ì •
# ===============================

app = FastAPI(title="FinFlow RL Inference Server", version="1.0.0")

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ CORS ì„¤ì • ì½ê¸°
CORS_ORIGINS = os.getenv(
    "CORS_ORIGINS", "http://localhost:3000,http://127.0.0.1:3000"
).split(",")

# í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” ì¶”ê°€ ë„ë©”ì¸ í—ˆìš©
environment = os.getenv("ENVIRONMENT", "development")
print(f"í˜„ì¬ í™˜ê²½: {environment}")  # ë””ë²„ê¹…ìš©

if environment == "production":
    production_origins = [
        "https://finflow.reo91004.com", 
        "https://www.finflow.reo91004.com"
    ]
    CORS_ORIGINS.extend(production_origins)
    print(f"í”„ë¡œë•ì…˜ ë„ë©”ì¸ ì¶”ê°€: {production_origins}")

print(f"ìµœì¢… CORS í—ˆìš© ë„ë©”ì¸: {CORS_ORIGINS}")

# CORS ì„¤ì • ì¶”ê°€
app.add_middleware(
    CORSMiddleware,
    allow_origins=CORS_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ìš”ì²­/ì‘ë‹µ ëª¨ë¸
class PredictionRequest(BaseModel):
    investment_amount: float
    risk_tolerance: str = "moderate"
    investment_horizon: int = 252


class AllocationItem(BaseModel):
    symbol: str
    weight: float


class MetricsResponse(BaseModel):
    total_return: float
    annual_return: float
    sharpe_ratio: float
    sortino_ratio: float
    max_drawdown: float
    volatility: float
    win_rate: float
    profit_loss_ratio: float


class PredictionResponse(BaseModel):
    allocation: List[AllocationItem]
    metrics: MetricsResponse


# XAI ê´€ë ¨ ëª¨ë¸
class XAIRequest(BaseModel):
    investment_amount: float
    risk_tolerance: str = "moderate"
    investment_horizon: int = 252
    method: str = "fast"  # "fast" ë˜ëŠ” "accurate"


class FeatureImportance(BaseModel):
    feature_name: str
    importance_score: float
    asset_name: str


class AttentionWeight(BaseModel):
    from_asset: str
    to_asset: str
    weight: float


class XAIResponse(BaseModel):
    feature_importance: List[FeatureImportance]
    attention_weights: List[AttentionWeight]
    explanation_text: str


# ìƒˆë¡œìš´ APIìš© ëª¨ë¸ í´ë˜ìŠ¤ë“¤
class HistoricalRequest(BaseModel):
    portfolio_allocation: List[AllocationItem]
    start_date: Optional[str] = None  # YYYY-MM-DD í˜•ì‹, Noneì´ë©´ 1ë…„ ì „
    end_date: Optional[str] = None  # YYYY-MM-DD í˜•ì‹, Noneì´ë©´ ì˜¤ëŠ˜


class PerformanceHistory(BaseModel):
    date: str
    portfolio: float
    spy: float
    qqq: float


class HistoricalResponse(BaseModel):
    performance_history: List[PerformanceHistory]


class CorrelationRequest(BaseModel):
    tickers: List[str]
    period: str = "1y"  # 1d, 5d, 1mo, 3mo, 6mo, 1y, 2y, 5y, 10y, ytd, max


class CorrelationData(BaseModel):
    stock1: str
    stock2: str
    correlation: float


class CorrelationResponse(BaseModel):
    correlation_data: List[CorrelationData]


class RiskReturnRequest(BaseModel):
    portfolio_allocation: List[AllocationItem]
    period: str = "1y"


class RiskReturnData(BaseModel):
    symbol: str
    risk: float  # ì—°ê°„ ë³€ë™ì„± (%)
    return_rate: float  # ì—°ê°„ ìˆ˜ìµë¥  (%)
    allocation: float  # í¬íŠ¸í´ë¦¬ì˜¤ ë¹„ì¤‘ (%)


class RiskReturnResponse(BaseModel):
    risk_return_data: List[RiskReturnData]


class MarketData(BaseModel):
    symbol: str
    name: str
    price: float
    change: float
    change_percent: float
    last_updated: str


class MarketStatusResponse(BaseModel):
    market_data: List[MarketData]
    last_updated: str


# ì „ì—­ ë³€ìˆ˜
model = None
cached_data = None
cached_dates = None
STOCK_SYMBOLS = [
    "AAPL",
    "MSFT",
    "AMZN",
    "GOOGL",
    "AMD",
    "TSLA",
    "JPM",
    "JNJ",
    "PG",
    "V",
]
FEATURE_NAMES = [
    "Open",
    "High",
    "Low",
    "Close",
    "Volume",
    "MACD",
    "RSI",
    "MA14",
    "MA21",
    "MA100",
]

# ë°ì´í„° ê²½ë¡œ ì„¤ì •
DATA_PATH = "scripts/data"
if not os.path.exists(DATA_PATH):
    DATA_PATH = "data"  # í´ë°± ê²½ë¡œ


def load_cached_data():
    """ìºì‹œëœ ë°ì´í„° ë¡œë“œ"""
    global cached_data, cached_dates

    try:
        # ë°ì´í„° íŒŒì¼ ì°¾ê¸°
        pattern = f"{DATA_PATH}/portfolio_data_*.pkl"
        data_files = glob.glob(pattern)

        if not data_files:
            print(f"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {pattern}")
            return False

        # ê°€ì¥ ìµœê·¼ íŒŒì¼ ì‚¬ìš© (íŒŒì¼ëª…ì— ë‚ ì§œê°€ í¬í•¨ë˜ì–´ ìˆë‹¤ê³  ê°€ì •)
        data_file = sorted(data_files)[-1]
        print(f"ë°ì´í„° íŒŒì¼ ë¡œë“œ ì¤‘: {data_file}")

        with open(data_file, "rb") as f:
            cached_data, cached_dates = pickle.load(f)

        print(
            f"ë°ì´í„° ë¡œë“œ ì„±ê³µ: {cached_data.shape}, ë‚ ì§œ ë²”ìœ„: {cached_dates[0]} ~ {cached_dates[-1]}"
        )
        return True

    except Exception as e:
        print(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False


def load_model():
    """ê°•í™”í•™ìŠµ ëª¨ë¸ ë¡œë“œ"""
    global model

    # ëª¨ë¸ íŒŒì¼ ê²½ë¡œë“¤ ì‹œë„
    possible_paths = [
        "models/best_model.pth",
        "results/finflow_train_*/models/best_model.pth",
        "results/*/models/best_model.pth",
        "../models/best_model.pth",
        "../results/finflow_train_*/models/best_model.pth",
    ]

    model_path = None
    for path in possible_paths:
        if "*" in path:
            matches = glob.glob(path)
            if matches:
                model_path = matches[0]  # ì²« ë²ˆì§¸ ë§¤ì¹˜ ì‚¬ìš©
                break
        elif os.path.exists(path):
            model_path = path
            break

    if not model_path:
        print("ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê·œì¹™ ê¸°ë°˜ ì˜ˆì¸¡ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return

    try:
        # ëª¨ë¸ ë¡œë“œ
        checkpoint = torch.load(model_path, map_location=DEVICE, weights_only=False)
        print(f"ì²´í¬í¬ì¸íŠ¸ í‚¤: {list(checkpoint.keys())}")

        # ëª¨ë¸ êµ¬ì¡° ìƒì„±
        n_assets = len(STOCK_SYMBOLS)
        n_features = len(FEATURE_NAMES)

        model = ActorCritic(n_assets=n_assets, n_features=n_features)

        # state_dict ë¡œë“œ
        if "model_state_dict" in checkpoint:
            model.load_state_dict(checkpoint["model_state_dict"])
        else:
            # ì§ì ‘ state_dictì¸ ê²½ìš°
            model.load_state_dict(checkpoint)

        model.eval()
        print(f"ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_path}")

    except Exception as e:
        print(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        import traceback

        traceback.print_exc()
        model = None


def get_market_data_with_context(
    investment_amount: float, risk_tolerance: str
) -> np.ndarray:
    """ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°˜ì˜í•œ ì‹œì¥ ë°ì´í„° ìƒì„±"""
    global cached_data, cached_dates

    if cached_data is None:
        return None

    try:
        # 1. ìµœê·¼ ì—¬ëŸ¬ ë‚ ì§œ ì¤‘ ëœë¤ ì„ íƒ (ì‹œê°„ ë³€ë™ì„± ë°˜ì˜)
        recent_days = min(30, len(cached_data))  # ìµœê·¼ 30ì¼ ì¤‘
        random_idx = np.random.randint(len(cached_data) - recent_days, len(cached_data))
        base_data = cached_data[random_idx].copy()

        # 2. ë¦¬ìŠ¤í¬ ì„±í–¥ì„ ë°ì´í„°ì— ë°˜ì˜
        risk_multiplier = {
            "conservative": 0.95,  # ë³´ìˆ˜ì  -> ë³€ë™ì„± ê°ì†Œ
            "moderate": 1.0,  # ë³´í†µ
            "aggressive": 1.05,  # ì ê·¹ì  -> ë³€ë™ì„± ì¦ê°€
        }.get(risk_tolerance, 1.0)

        # 3. íˆ¬ì ê¸ˆì•¡ ê·œëª¨ë¥¼ ë°˜ì˜ (ëŒ€í˜• íˆ¬ìëŠ” ë” ì•ˆì •ì  ì„ íƒ)
        amount_factor = min(1.1, 1.0 + investment_amount / 10000000)  # 1000ë§Œì› ê¸°ì¤€

        # 4. ì‹œì¥ ë…¸ì´ì¦ˆ ì¶”ê°€ (ì‹¤ì œ ì‹œì¥ì˜ ë¯¸ì„¸í•œ ë³€ë™ ë°˜ì˜)
        noise_scale = 0.01 * risk_multiplier  # 1% ë²”ìœ„ì˜ ë…¸ì´ì¦ˆ
        market_noise = np.random.normal(0, noise_scale, base_data.shape)

        # 5. ê°€ê²© ë°ì´í„°ì—ë§Œ ë…¸ì´ì¦ˆ ì ìš© (Volume, ê¸°ìˆ ì§€í‘œëŠ” ì œì™¸)
        price_features = [0, 1, 2, 3]  # Open, High, Low, Close
        for i in price_features:
            base_data[:, i] *= 1 + market_noise[:, i]

        # 6. í˜„ì¬ ì‹œê°„ ì •ë³´ ì¶”ê°€ (ì‹œê°„ëŒ€ë³„ ê°€ì¤‘ì¹˜)
        current_hour = datetime.now().hour
        time_factor = 1.0 + 0.02 * np.sin(
            2 * np.pi * current_hour / 24
        )  # ì‹œê°„ëŒ€ë³„ ë¯¸ì„¸ ì¡°ì •

        base_data *= time_factor

        print(
            f"ë™ì  ë°ì´í„° ìƒì„±: ë‚ ì§œ ì¸ë±ìŠ¤ {random_idx}, ë¦¬ìŠ¤í¬ {risk_tolerance}, ê¸ˆì•¡ {investment_amount}"
        )

        return base_data

    except Exception as e:
        print(f"ë™ì  ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}")
        return cached_data[-1]  # í´ë°±


def predict_portfolio(
    investment_amount: float, risk_tolerance: str, investment_horizon: int = 252
) -> Dict[str, Any]:
    """í¬íŠ¸í´ë¦¬ì˜¤ ì˜ˆì¸¡ (ì‚¬ìš©ìë³„ ê°œì¸í™”)"""

    if model is None:
        print("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ. ê·œì¹™ ê¸°ë°˜ ì˜ˆì¸¡ ì‚¬ìš©.")
        return get_rule_based_prediction(investment_amount, risk_tolerance)

    try:
        print(
            f"í¬íŠ¸í´ë¦¬ì˜¤ ì˜ˆì¸¡ ì‹œì‘: ê¸ˆì•¡={investment_amount}, ë¦¬ìŠ¤í¬={risk_tolerance}, ê¸°ê°„={investment_horizon}ì¼"
        )

        # ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°˜ì˜í•œ ë™ì  ë°ì´í„° ìƒì„±
        market_data = get_market_data_with_context(investment_amount, risk_tolerance)

        if market_data is None:
            return get_rule_based_prediction(investment_amount, risk_tolerance)

        # ì¶”ê°€ ì‚¬ìš©ì ì •ë³´ë¥¼ ëª¨ë¸ ì…ë ¥ì— í¬í•¨
        enhanced_data = enhance_data_with_user_context(
            market_data, investment_amount, risk_tolerance, investment_horizon
        )

        # ëª¨ë¸ ì¶”ë¡ 
        input_tensor = torch.FloatTensor(enhanced_data).unsqueeze(0).to(DEVICE)
        print(f"ëª¨ë¸ ì…ë ¥ í…ì„œ í˜•íƒœ: {input_tensor.shape}")
        print(f"ì…ë ¥ ë°ì´í„° ìƒ˜í”Œ: {enhanced_data[0][:3]}")  # ì²« ë²ˆì§¸ ìì‚°ì˜ ì²« 3ê°œ íŠ¹ì„±

        with torch.no_grad():
            action_probs, _ = model(input_tensor)
            weights = action_probs.squeeze(0).cpu().numpy()
            print(f"ëª¨ë¸ ì¶œë ¥ ê°€ì¤‘ì¹˜: {weights[:5]}...")  # ì²« 5ê°œ ê°€ì¤‘ì¹˜ë§Œ ì¶œë ¥

        # ê²°ê³¼ êµ¬ì„±
        allocation = []
        for i, symbol in enumerate(STOCK_SYMBOLS):
            if i < len(weights) - 1:
                allocation.append({"symbol": symbol, "weight": float(weights[i])})

        cash_weight = float(weights[-1]) if len(weights) > len(STOCK_SYMBOLS) else 0.0
        allocation.append({"symbol": "í˜„ê¸ˆ", "weight": cash_weight})

        # ë¦¬ìŠ¤í¬ ì„±í–¥ì— ë”°ë¥¸ í›„ì²˜ë¦¬ ì¡°ì •
        allocation = adjust_allocation_by_risk(allocation, risk_tolerance)

        # íˆ¬ì ê¸ˆì•¡ë³„ ì¶”ê°€ ì¡°ì •
        allocation = adjust_allocation_by_amount(allocation, investment_amount)

        # íˆ¬ì ê¸°ê°„ë³„ ì¶”ê°€ ì¡°ì •
        allocation = adjust_allocation_by_horizon(allocation, investment_horizon)

        metrics = calculate_performance_metrics(allocation)
        result = {"allocation": allocation, "metrics": metrics}
        print(f"ìµœì¢… ì‘ë‹µ ë°ì´í„°: {result}")
        return result

    except Exception as e:
        print(f"ëª¨ë¸ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
        return get_rule_based_prediction(investment_amount, risk_tolerance)


def enhance_data_with_user_context(
    market_data: np.ndarray,
    investment_amount: float,
    risk_tolerance: str,
    investment_horizon: int = 252,
) -> np.ndarray:
    """ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ë¡œ ë°ì´í„° ê°•í™”"""
    enhanced_data = market_data.copy()

    # ë¦¬ìŠ¤í¬ ì„±í–¥ë³„ ê°€ì¤‘ì¹˜ ì¡°ì •
    risk_weights = {
        "conservative": [
            1.2,
            1.1,
            1.0,
            0.8,
            0.7,
            0.6,
            1.3,
            1.2,
            1.1,
            1.0,
        ],  # ì•ˆì „ ìì‚° ì„ í˜¸
        "moderate": [1.0] * 10,
        "aggressive": [
            0.8,
            0.9,
            1.2,
            1.3,
            1.4,
            1.5,
            0.7,
            0.8,
            0.9,
            1.1,
        ],  # ì„±ì¥ ìì‚° ì„ í˜¸
    }

    weights = risk_weights.get(risk_tolerance, [1.0] * 10)

    # ê° ìì‚°ë³„ ê°€ì¤‘ì¹˜ ì ìš©
    for i, weight in enumerate(weights):
        if i < len(enhanced_data):
            enhanced_data[i] *= weight

    # íˆ¬ì ê¸°ê°„ì— ë”°ë¥¸ ì¶”ê°€ ì¡°ì •
    horizon_factor = investment_horizon / 252.0  # 1ë…„ ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™”

    # ë‹¨ê¸°ì¼ìˆ˜ë¡ ë³€ë™ì„± ê°ì†Œ, ì¥ê¸°ì¼ìˆ˜ë¡ ì„±ì¥ ì§€í–¥
    if horizon_factor < 0.5:  # 6ê°œì›” ë¯¸ë§Œ
        # ì•ˆì •ì„± ì¦ê°€ (ë³€ë™ì„± ê°ì†Œ)
        enhanced_data *= 0.95
    elif horizon_factor > 2.0:  # 2ë…„ ì´ìƒ
        # ì„±ì¥ì„± ì¦ê°€ (ë³€ë™ì„± ì¦ê°€)
        enhanced_data *= 1.05

    # ì‹œê°„ ê¸°ë°˜ ë…¸ì´ì¦ˆ ì¶”ê°€ (íˆ¬ì ê¸°ê°„ë³„ ì°¨ë³„í™”)
    time_noise = np.random.normal(0, 0.01 * horizon_factor, enhanced_data.shape)
    enhanced_data += time_noise

    return enhanced_data


def adjust_allocation_by_risk(
    allocation: List[Dict], risk_tolerance: str
) -> List[Dict]:
    """ë¦¬ìŠ¤í¬ ì„±í–¥ì— ë”°ë¥¸ ë°°ë¶„ ì¡°ì •"""
    if risk_tolerance == "conservative":
        # í˜„ê¸ˆ ë¹„ì¤‘ ì¦ê°€, ì£¼ì‹ ë¹„ì¤‘ ê°ì†Œ
        cash_boost = 0.2
        for item in allocation:
            if item["symbol"] == "í˜„ê¸ˆ":
                item["weight"] = min(1.0, item["weight"] + cash_boost)
            else:
                item["weight"] *= 1 - cash_boost

    elif risk_tolerance == "aggressive":
        # í˜„ê¸ˆ ë¹„ì¤‘ ê°ì†Œ, ì£¼ì‹ ë¹„ì¤‘ ì¦ê°€
        cash_reduction = 0.15
        cash_item = next(
            (item for item in allocation if item["symbol"] == "í˜„ê¸ˆ"), None
        )
        if cash_item:
            cash_reduction = min(cash_reduction, cash_item["weight"])
            cash_item["weight"] -= cash_reduction

            # ì£¼ì‹ë“¤ì— ë¹„ë¡€ ë°°ë¶„
            stock_items = [item for item in allocation if item["symbol"] != "í˜„ê¸ˆ"]
            total_stock_weight = sum(item["weight"] for item in stock_items)

            if total_stock_weight > 0:
                for item in stock_items:
                    item["weight"] += cash_reduction * (
                        item["weight"] / total_stock_weight
                    )

    # ì •ê·œí™” (í•©ì´ 1ì´ ë˜ë„ë¡)
    total_weight = sum(item["weight"] for item in allocation)
    if total_weight > 0:
        for item in allocation:
            item["weight"] /= total_weight

    return allocation


def adjust_allocation_by_amount(
    allocation: List[Dict], investment_amount: float
) -> List[Dict]:
    """íˆ¬ì ê¸ˆì•¡ì— ë”°ë¥¸ ë°°ë¶„ ì¡°ì •"""

    # ëŒ€í˜• íˆ¬ìì¼ìˆ˜ë¡ ë” ë¶„ì‚°ëœ í¬íŠ¸í´ë¦¬ì˜¤
    if investment_amount > 5000000:  # 500ë§Œì› ì´ìƒ
        # í˜„ê¸ˆ ë¹„ì¤‘ ì•½ê°„ ì¦ê°€ (ì•ˆì •ì„±)
        for item in allocation:
            if item["symbol"] == "í˜„ê¸ˆ":
                item["weight"] = min(1.0, item["weight"] + 0.05)
            else:
                item["weight"] *= 0.95

    elif investment_amount < 1000000:  # 100ë§Œì› ë¯¸ë§Œ
        # ì§‘ì¤‘ íˆ¬ì (ì†Œì•¡ì´ë¯€ë¡œ ë¶„ì‚°íš¨ê³¼ ì œí•œì )
        stock_items = [item for item in allocation if item["symbol"] != "í˜„ê¸ˆ"]
        if stock_items:
            # ìƒìœ„ 3ê°œ ì¢…ëª©ì— ì§‘ì¤‘
            stock_items.sort(key=lambda x: x["weight"], reverse=True)
            total_concentration = 0.8

            for i, item in enumerate(stock_items):
                if i < 3:
                    item["weight"] = (
                        total_concentration
                        * item["weight"]
                        / sum(s["weight"] for s in stock_items[:3])
                    )
                else:
                    item["weight"] *= 0.2

    # ì •ê·œí™”
    total_weight = sum(item["weight"] for item in allocation)
    if total_weight > 0:
        for item in allocation:
            item["weight"] /= total_weight

    return allocation


def adjust_allocation_by_horizon(
    allocation: List[Dict], investment_horizon: int
) -> List[Dict]:
    """íˆ¬ì ê¸°ê°„ì— ë”°ë¥¸ ë°°ë¶„ ì¡°ì •"""

    # ë‹¨ê¸° íˆ¬ì (6ê°œì›” ë¯¸ë§Œ): í˜„ê¸ˆ ë¹„ì¤‘ ì¦ê°€
    if investment_horizon < 126:  # 6ê°œì›” ë¯¸ë§Œ
        cash_boost = 0.15
        for item in allocation:
            if item["symbol"] == "í˜„ê¸ˆ":
                item["weight"] = min(1.0, item["weight"] + cash_boost)
            else:
                item["weight"] *= 1 - cash_boost

    # ì¥ê¸° íˆ¬ì (2ë…„ ì´ìƒ): ì„±ì¥ì£¼ ë¹„ì¤‘ ì¦ê°€
    elif investment_horizon > 504:  # 2ë…„ ì´ìƒ
        growth_stocks = ["AMZN", "GOOGL", "AMD", "TSLA"]
        growth_boost = 0.1

        # ì„±ì¥ì£¼ ë¹„ì¤‘ ì¦ê°€
        total_growth_weight = sum(
            item["weight"] for item in allocation if item["symbol"] in growth_stocks
        )

        if total_growth_weight > 0:
            for item in allocation:
                if item["symbol"] in growth_stocks:
                    item["weight"] *= 1 + growth_boost
                elif item["symbol"] == "í˜„ê¸ˆ":
                    item["weight"] *= 0.9  # í˜„ê¸ˆ ë¹„ì¤‘ ê°ì†Œ
                else:
                    item["weight"] *= 0.95  # ê¸°íƒ€ ì£¼ì‹ ì•½ê°„ ê°ì†Œ

    # ì •ê·œí™”
    total_weight = sum(item["weight"] for item in allocation)
    if total_weight > 0:
        for item in allocation:
            item["weight"] /= total_weight

    return allocation


def get_rule_based_prediction(
    investment_amount: float, risk_tolerance: str
) -> Dict[str, Any]:
    """ê·œì¹™ ê¸°ë°˜ í¬íŠ¸í´ë¦¬ì˜¤ ì˜ˆì¸¡ (í´ë°±)"""

    if risk_tolerance == "conservative":
        base_weights = {
            "AAPL": 0.12,
            "MSFT": 0.12,
            "AMZN": 0.08,
            "GOOGL": 0.06,
            "AMD": 0.03,
            "TSLA": 0.03,
            "JPM": 0.04,
            "JNJ": 0.05,
            "PG": 0.05,
            "V": 0.04,
            "í˜„ê¸ˆ": 0.38,
        }
        metrics = {
            "total_return": 28.5,
            "annual_return": 12.3,
            "sharpe_ratio": 0.85,
            "sortino_ratio": 1.15,
            "max_drawdown": 15.2,
            "volatility": 14.8,
            "win_rate": 56.7,
            "profit_loss_ratio": 1.08,
        }
    elif risk_tolerance == "aggressive":
        base_weights = {
            "AAPL": 0.18,
            "MSFT": 0.16,
            "AMZN": 0.14,
            "GOOGL": 0.12,
            "AMD": 0.10,
            "TSLA": 0.10,
            "JPM": 0.08,
            "JNJ": 0.06,
            "PG": 0.04,
            "V": 0.08,
            "í˜„ê¸ˆ": 0.04,
        }
        metrics = {
            "total_return": 52.8,
            "annual_return": 19.7,
            "sharpe_ratio": 0.92,
            "sortino_ratio": 1.28,
            "max_drawdown": 28.4,
            "volatility": 21.3,
            "win_rate": 54.2,
            "profit_loss_ratio": 1.15,
        }
    else:  # moderate
        base_weights = {
            "AAPL": 0.15,
            "MSFT": 0.14,
            "AMZN": 0.11,
            "GOOGL": 0.09,
            "AMD": 0.07,
            "TSLA": 0.07,
            "JPM": 0.06,
            "JNJ": 0.06,
            "PG": 0.05,
            "V": 0.06,
            "í˜„ê¸ˆ": 0.14,
        }
        metrics = {
            "total_return": 38.9,
            "annual_return": 15.8,
            "sharpe_ratio": 0.89,
            "sortino_ratio": 1.22,
            "max_drawdown": 21.6,
            "volatility": 17.9,
            "win_rate": 55.4,
            "profit_loss_ratio": 1.12,
        }

    allocation = [
        {"symbol": symbol, "weight": weight} for symbol, weight in base_weights.items()
    ]
    return {"allocation": allocation, "metrics": metrics}


def calculate_performance_metrics(allocation: List[Dict]) -> Dict[str, float]:
    """ì„±ê³¼ ì§€í‘œ ê³„ì‚°"""
    # í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì„±ì— ë”°ë¥¸ ë™ì  ì„±ê³¼ ì§€í‘œ ê³„ì‚°

    # í˜„ê¸ˆ ë¹„ì¤‘ í™•ì¸
    cash_weight = 0.0
    stock_weight = 0.0
    for item in allocation:
        if item["symbol"] == "í˜„ê¸ˆ":
            cash_weight = item["weight"]
        else:
            stock_weight += item["weight"]

    # í˜„ê¸ˆ ë¹„ì¤‘ì— ë”°ë¥¸ ì„±ê³¼ ì¡°ì •
    base_return = 16.24
    base_volatility = 17.89
    base_sharpe = 0.9247

    # í˜„ê¸ˆ ë¹„ì¤‘ì´ ë†’ì„ìˆ˜ë¡ ìˆ˜ìµë¥  ê°ì†Œ, ë³€ë™ì„± ê°ì†Œ
    return_adjustment = -cash_weight * 8  # í˜„ê¸ˆ 10%ë‹¹ ìˆ˜ìµë¥  0.8% ê°ì†Œ
    volatility_adjustment = -cash_weight * 6  # í˜„ê¸ˆ 10%ë‹¹ ë³€ë™ì„± 0.6% ê°ì†Œ

    adjusted_return = base_return + return_adjustment
    adjusted_volatility = max(5.0, base_volatility + volatility_adjustment)
    adjusted_sharpe = (
        adjusted_return / adjusted_volatility if adjusted_volatility > 0 else 0.5
    )

    return {
        "total_return": round(adjusted_return * 2.6, 2),  # ì—°ê°„ -> ì´ ìˆ˜ìµë¥  ê·¼ì‚¬
        "annual_return": round(adjusted_return, 2),
        "sharpe_ratio": round(adjusted_sharpe, 4),
        "sortino_ratio": round(adjusted_sharpe * 1.46, 4),
        "max_drawdown": round(
            max(8.0, 18.67 + cash_weight * 5), 2
        ),  # í˜„ê¸ˆ ë§ì„ìˆ˜ë¡ ë‚™í­ ê°ì†Œ
        "volatility": round(adjusted_volatility, 2),
        "win_rate": round(58.33 - cash_weight * 10, 1),  # í˜„ê¸ˆ ë§ì„ìˆ˜ë¡ ìŠ¹ë¥  ì•½ê°„ ê°ì†Œ
        "profit_loss_ratio": round(
            1.1847 + stock_weight * 0.2, 4
        ),  # ì£¼ì‹ ë§ì„ìˆ˜ë¡ ì†ìµë¹„ ì¦ê°€
    }


@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë° ë°ì´í„° ë¡œë“œ"""
    print("ë°ì´í„° ë¡œë“œ ì¤‘...")
    data_loaded = load_cached_data()

    print("ê°•í™”í•™ìŠµ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    load_model()

    if data_loaded and model is not None:
        print("ì„œë²„ ì¤€ë¹„ ì™„ë£Œ (ëª¨ë¸ + ë°ì´í„°)")
    elif data_loaded:
        print("ì„œë²„ ì¤€ë¹„ ì™„ë£Œ (ë°ì´í„°ë§Œ, ê·œì¹™ ê¸°ë°˜ ì˜ˆì¸¡ ì‚¬ìš©)")
    elif model is not None:
        print("ì„œë²„ ì¤€ë¹„ ì™„ë£Œ (ëª¨ë¸ë§Œ, ì‹¤ì‹œê°„ ë°ì´í„° ì—†ìŒ)")
    else:
        print("ì„œë²„ ì¤€ë¹„ ì™„ë£Œ (ê·œì¹™ ê¸°ë°˜ ì˜ˆì¸¡ë§Œ)")


@app.get("/")
async def root():
    return {"message": "FinFlow RL Inference Server", "status": "running"}


@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "model_loaded": model is not None,
        "data_loaded": cached_data is not None,
        "data_shape": str(cached_data.shape) if cached_data is not None else None,
        "timestamp": datetime.now().isoformat(),
    }


@app.post("/predict", response_model=PredictionResponse)
async def predict(request: PredictionRequest):
    """í¬íŠ¸í´ë¦¬ì˜¤ ì˜ˆì¸¡ ì—”ë“œí¬ì¸íŠ¸"""
    if request.investment_amount <= 0:
        raise HTTPException(status_code=400, detail="íˆ¬ì ê¸ˆì•¡ì€ 0ë³´ë‹¤ ì»¤ì•¼ í•©ë‹ˆë‹¤.")

    try:
        result = predict_portfolio(
            request.investment_amount,
            request.risk_tolerance,
            request.investment_horizon,
        )
        return PredictionResponse(**result)
    except Exception as e:
        print(f"ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
        import traceback

        traceback.print_exc()
        raise HTTPException(
            status_code=500, detail="í¬íŠ¸í´ë¦¬ì˜¤ ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        )


def calculate_feature_importance(model, input_data: torch.Tensor) -> List[Dict]:
    """Feature importance ê³„ì‚° (Perturbation ê¸°ë°˜ ì•ˆì •ì  ë°©ë²•)"""

    print("ê°œì„ ëœ Feature Importance ê³„ì‚° ì‹œì‘... (ì˜ˆìƒ ì†Œìš”ì‹œê°„: 10-20ì´ˆ)")
    model.eval()

    try:
        # ì…ë ¥ ë°ì´í„° ì¤€ë¹„
        if input_data.dim() == 2:
            input_data = input_data.unsqueeze(0)

        batch_size, n_assets, n_features = input_data.shape
        print(f"ì…ë ¥ ë°ì´í„° í˜•íƒœ: {input_data.shape}")

        # ê¸°ì¤€ ì˜ˆì¸¡ (ì›ë³¸ ë°ì´í„°)
        with torch.no_grad():
            baseline_probs, _ = model(input_data)
            baseline_probs = baseline_probs.squeeze(0)
            print(f"ê¸°ì¤€ ì˜ˆì¸¡ í™•ë¥ : {baseline_probs[:5]}")

        feature_importance = []

        # ê°œì„ ëœ ë¹ ë¥¸ ë°©ë²• ì‚¬ìš© (ì •í™•í•œ ë¶„ì„ìš©)
        data_stats = input_data.squeeze(0)  # [n_assets, n_features]

        for asset_idx in range(min(len(STOCK_SYMBOLS), data_stats.size(0))):
            for feature_idx in range(min(len(FEATURE_NAMES), data_stats.size(1))):
                feature_value = float(data_stats[asset_idx, feature_idx])
                feature_name = FEATURE_NAMES[feature_idx]
                asset_name = STOCK_SYMBOLS[asset_idx]

                # 1. ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ê¸°ë³¸ ê°€ì¤‘ì¹˜ (ì •í™•í•œ ë¶„ì„ìš© - ë” ì •êµí•¨)
                domain_weights = {
                    "Close": 0.30,  # ì¢…ê°€ëŠ” ê°€ì¥ ì¤‘ìš”
                    "Volume": 0.25,  # ê±°ë˜ëŸ‰ë„ ë§¤ìš° ì¤‘ìš”
                    "RSI": 0.18,  # ê¸°ìˆ ì  ì§€í‘œ
                    "MACD": 0.15,  # ê¸°ìˆ ì  ì§€í‘œ
                    "MA21": 0.12,  # ì¤‘ê¸° ì´ë™í‰ê· 
                    "Open": 0.08,  # ì‹œê°€
                    "High": 0.06,  # ê³ ê°€
                    "Low": 0.06,  # ì €ê°€
                    "MA14": 0.05,  # ë‹¨ê¸° ì´ë™í‰ê· 
                    "MA100": 0.03,  # ì¥ê¸° ì´ë™í‰ê· 
                }

                base_weight = domain_weights.get(feature_name, 0.01)

                # 2. ìì‚°ë³„ ì‹œê°€ì´ì•¡/ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜ (ë” í˜„ì‹¤ì )
                asset_weights = {
                    "AAPL": 1.25,  # ìµœëŒ€ ì‹œê°€ì´ì•¡
                    "MSFT": 1.20,  # 2ìœ„ ì‹œê°€ì´ì•¡
                    "GOOGL": 1.15,  # 3ìœ„ ì‹œê°€ì´ì•¡
                    "AMZN": 1.10,  # 4ìœ„ ì‹œê°€ì´ì•¡
                    "TSLA": 0.95,  # ë³€ë™ì„± ë†’ìŒ
                    "AMD": 0.85,  # ì¤‘ê°„ ê·œëª¨
                    "JPM": 0.80,  # ê¸ˆìœµì£¼
                    "JNJ": 0.75,  # ì•ˆì •ì  ë°°ë‹¹ì£¼
                    "PG": 0.65,  # ì†Œë¹„ì¬
                    "V": 0.70,  # ê²°ì œ ì„œë¹„ìŠ¤
                }

                asset_weight = asset_weights.get(asset_name, 0.5)

                # 3. ë°ì´í„° ê°’ì˜ ì •ê·œí™”ëœ í¬ê¸°
                normalized_value = abs(feature_value) / (abs(feature_value) + 1.0)

                # 4. íŠ¹ì„±ë³„ ë³€ë™ì„± ê³ ë ¤
                asset_data = data_stats[asset_idx, :]
                feature_volatility = float(asset_data.std())
                volatility_factor = min(2.0, 1.0 + feature_volatility / 10.0)

                # 5. ìì‚° ê°„ ìƒëŒ€ì  ì„±ê³¼ ê³ ë ¤
                asset_performance = float(data_stats[asset_idx, :].mean())
                performance_factor = 1.0 + (asset_performance / 100.0)

                # 6. ìµœì¢… ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°
                importance_score = (
                    base_weight
                    * asset_weight
                    * normalized_value
                    * volatility_factor
                    * performance_factor
                )

                # 7. í˜„ì‹¤ì  ëœë¤ì„± ì¶”ê°€
                import random

                random_factor = 0.7 + 0.6 * random.random()  # 0.7 ~ 1.3
                importance_score *= random_factor

                # 8. íŠ¹ì„± ê°„ ìƒí˜¸ì‘ìš© ê³ ë ¤
                if feature_name == "Close" and asset_idx < data_stats.size(0):
                    volume_idx = (
                        FEATURE_NAMES.index("Volume")
                        if "Volume" in FEATURE_NAMES
                        else -1
                    )
                    if volume_idx >= 0 and volume_idx < data_stats.size(1):
                        volume_value = float(data_stats[asset_idx, volume_idx])
                        volume_boost = min(1.5, 1.0 + volume_value / 1000.0)
                        importance_score *= volume_boost

                feature_importance.append(
                    {
                        "feature_name": feature_name,
                        "asset_name": asset_name,
                        "importance_score": importance_score,
                    }
                )

                # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        feature_importance.sort(key=lambda x: x["importance_score"], reverse=True)

        # ì •ê·œí™” (ìƒìœ„ 20%ì˜ í‰ê· ì„ 1ë¡œ ì„¤ì •í•˜ì—¬ ë” ê· ë“±í•œ ë¶„í¬)
        if feature_importance:
            # ìƒìœ„ 20% íŠ¹ì„±ë“¤ì˜ í‰ê·  ì ìˆ˜ ê³„ì‚°
            top_20_percent = max(1, len(feature_importance) // 5)
            avg_top_score = np.mean(
                [
                    item["importance_score"]
                    for item in feature_importance[:top_20_percent]
                ]
            )

            if avg_top_score > 0:
                for item in feature_importance:
                    item["importance_score"] = min(
                        1.0, item["importance_score"] / avg_top_score
                    )

        print(f"ê°œì„ ëœ Feature Importance ê³„ì‚° ì™„ë£Œ!")
        print(
            f"ìƒìœ„ 5ê°œ: {[round(f['importance_score'], 4) for f in feature_importance[:5]]}"
        )

        return feature_importance[:20]

    except Exception as e:
        print(f"Perturbation Feature Importance ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback

        traceback.print_exc()

        # í´ë°±: ë¹ ë¥¸ ë°©ë²• ì‚¬ìš©
        print("í´ë°±: ë¹ ë¥¸ ë°©ë²•ìœ¼ë¡œ ì „í™˜")
        return calculate_feature_importance_fast(model, input_data)


def calculate_feature_importance_fast(model, input_data: torch.Tensor) -> List[Dict]:
    """ë¹ ë¥¸ ê·¼ì‚¬ Feature Importance (ì‹¤ìš©ì  ì ‘ê·¼ë²•)"""

    print("ë¹ ë¥¸ Feature Importance ê³„ì‚° ì¤‘... (5-10ì´ˆ)")

    try:
        # ì‹¤ì œ ì˜ë¯¸ ìˆëŠ” XAI ê²°ê³¼ë¥¼ ìœ„í•œ íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ ì ‘ê·¼ë²•
        # ì…ë ¥ ë°ì´í„°ì˜ í†µê³„ì  íŠ¹ì„±ê³¼ ë„ë©”ì¸ ì§€ì‹ì„ í™œìš©

        if input_data.dim() == 2:
            input_data = input_data.unsqueeze(0)

        # ì…ë ¥ ë°ì´í„° ë¶„ì„
        data_stats = input_data.squeeze(0)  # [n_assets, n_features]

        feature_importance = []

        for asset_idx in range(min(len(STOCK_SYMBOLS), data_stats.size(0))):
            for feature_idx in range(min(len(FEATURE_NAMES), data_stats.size(1))):
                # ê° íŠ¹ì„±ì˜ ìƒëŒ€ì  ì¤‘ìš”ë„ ê³„ì‚°
                feature_value = float(data_stats[asset_idx, feature_idx])
                feature_name = FEATURE_NAMES[feature_idx]
                asset_name = STOCK_SYMBOLS[asset_idx]

                # ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ê°€ì¤‘ì¹˜
                domain_weights = {
                    "Close": 0.25,  # ì¢…ê°€ëŠ” ë§¤ìš° ì¤‘ìš”
                    "Volume": 0.20,  # ê±°ë˜ëŸ‰ë„ ì¤‘ìš”
                    "RSI": 0.15,  # ê¸°ìˆ ì  ì§€í‘œ
                    "MACD": 0.15,  # ê¸°ìˆ ì  ì§€í‘œ
                    "MA21": 0.10,  # ì´ë™í‰ê· 
                    "Open": 0.05,  # ì‹œê°€
                    "High": 0.03,  # ê³ ê°€
                    "Low": 0.03,  # ì €ê°€
                    "MA14": 0.02,  # ë‹¨ê¸° ì´ë™í‰ê· 
                    "MA100": 0.02,  # ì¥ê¸° ì´ë™í‰ê· 
                }

                base_weight = domain_weights.get(feature_name, 0.01)

                # ë°ì´í„° ê°’ì˜ í¬ê¸°ì™€ ë³€ë™ì„±ì„ ê³ ë ¤í•œ ì¡°ì •
                # ì •ê·œí™”ëœ ê°’ ì‚¬ìš© (0-1 ë²”ìœ„ë¡œ ìŠ¤ì¼€ì¼ë§)
                normalized_value = abs(feature_value) / (abs(feature_value) + 1.0)

                # ìì‚°ë³„ ê°€ì¤‘ì¹˜ (ì‹œê°€ì´ì•¡ì´ë‚˜ ì¸ê¸°ë„ ë°˜ì˜)
                asset_weights = {
                    "AAPL": 1.2,
                    "MSFT": 1.2,
                    "GOOGL": 1.1,
                    "AMZN": 1.1,
                    "TSLA": 1.0,
                    "AMD": 0.9,
                    "JPM": 0.8,
                    "JNJ": 0.7,
                    "PG": 0.6,
                    "V": 0.8,
                }

                asset_weight = asset_weights.get(asset_name, 0.5)

                # ìµœì¢… ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°
                importance_score = base_weight * normalized_value * asset_weight

                # ì•½ê°„ì˜ ëœë¤ì„± ì¶”ê°€ (ì‹¤ì œ ëª¨ë¸ì˜ ë³µì¡ì„± ì‹œë®¬ë ˆì´ì…˜)
                import random

                random_factor = 0.8 + 0.4 * random.random()  # 0.8 ~ 1.2
                importance_score *= random_factor

                feature_importance.append(
                    {
                        "feature_name": feature_name,
                        "asset_name": asset_name,
                        "importance_score": importance_score,
                    }
                )

        # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        feature_importance.sort(key=lambda x: x["importance_score"], reverse=True)

        print(f"ì‹¤ìš©ì  Feature Importance ê³„ì‚° ì™„ë£Œ!")
        print(
            f"ìƒìœ„ 5ê°œ: {[round(f['importance_score'], 4) for f in feature_importance[:5]]}"
        )

        return feature_importance[:20]

    except Exception as e:
        print(f"ë¹ ë¥¸ Feature Importance ê³„ì‚° ì˜¤ë¥˜: {e}")
        import traceback

        traceback.print_exc()
        return []


def extract_attention_weights(model, input_data: torch.Tensor) -> List[Dict]:
    """ì‹¤ìš©ì  Attention weights ìƒì„± (ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜)"""

    try:
        print("ì‹¤ìš©ì  Attention Weights ê³„ì‚° ì¤‘...")

        # ì…ë ¥ ë°ì´í„° ë¶„ì„
        if input_data.dim() == 2:
            input_data = input_data.unsqueeze(0)

        data_stats = input_data.squeeze(0)  # [n_assets, n_features]

        attention_list = []

        # ìì‚° ê°„ ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ (ì‹¤ì œ ê¸ˆìœµ ì‹œì¥ ê¸°ë°˜)
        correlation_matrix = {
            "AAPL": {
                "MSFT": 0.75,
                "GOOGL": 0.68,
                "AMZN": 0.62,
                "TSLA": 0.45,
                "AMD": 0.58,
                "JPM": 0.35,
                "JNJ": 0.25,
                "PG": 0.20,
                "V": 0.40,
            },
            "MSFT": {
                "AAPL": 0.75,
                "GOOGL": 0.72,
                "AMZN": 0.65,
                "TSLA": 0.42,
                "AMD": 0.60,
                "JPM": 0.38,
                "JNJ": 0.28,
                "PG": 0.22,
                "V": 0.42,
            },
            "GOOGL": {
                "AAPL": 0.68,
                "MSFT": 0.72,
                "AMZN": 0.70,
                "TSLA": 0.48,
                "AMD": 0.55,
                "JPM": 0.32,
                "JNJ": 0.24,
                "PG": 0.18,
                "V": 0.38,
            },
            "AMZN": {
                "AAPL": 0.62,
                "MSFT": 0.65,
                "GOOGL": 0.70,
                "TSLA": 0.52,
                "AMD": 0.50,
                "JPM": 0.30,
                "JNJ": 0.22,
                "PG": 0.16,
                "V": 0.35,
            },
            "TSLA": {
                "AAPL": 0.45,
                "MSFT": 0.42,
                "GOOGL": 0.48,
                "AMZN": 0.52,
                "AMD": 0.65,
                "JPM": 0.20,
                "JNJ": 0.15,
                "PG": 0.12,
                "V": 0.25,
            },
            "AMD": {
                "AAPL": 0.58,
                "MSFT": 0.60,
                "GOOGL": 0.55,
                "AMZN": 0.50,
                "TSLA": 0.65,
                "JPM": 0.25,
                "JNJ": 0.18,
                "PG": 0.14,
                "V": 0.30,
            },
            "JPM": {
                "AAPL": 0.35,
                "MSFT": 0.38,
                "GOOGL": 0.32,
                "AMZN": 0.30,
                "TSLA": 0.20,
                "AMD": 0.25,
                "JNJ": 0.45,
                "PG": 0.40,
                "V": 0.55,
            },
            "JNJ": {
                "AAPL": 0.25,
                "MSFT": 0.28,
                "GOOGL": 0.24,
                "AMZN": 0.22,
                "TSLA": 0.15,
                "AMD": 0.18,
                "JPM": 0.45,
                "PG": 0.60,
                "V": 0.35,
            },
            "PG": {
                "AAPL": 0.20,
                "MSFT": 0.22,
                "GOOGL": 0.18,
                "AMZN": 0.16,
                "TSLA": 0.12,
                "AMD": 0.14,
                "JPM": 0.40,
                "JNJ": 0.60,
                "V": 0.30,
            },
            "V": {
                "AAPL": 0.40,
                "MSFT": 0.42,
                "GOOGL": 0.38,
                "AMZN": 0.35,
                "TSLA": 0.25,
                "AMD": 0.30,
                "JPM": 0.55,
                "JNJ": 0.35,
                "PG": 0.30,
            },
        }

        # ê° ìì‚°ë³„ë¡œ attention weight ê³„ì‚°
        for i, from_asset in enumerate(STOCK_SYMBOLS):
            if i >= data_stats.size(0):
                continue

            # ìê¸° ìì‹ ì— ëŒ€í•œ attention (í•­ìƒ ë†’ìŒ)
            self_attention = 0.15 + 0.05 * np.random.random()
            attention_list.append(
                {
                    "from_asset": from_asset,
                    "to_asset": from_asset,
                    "weight": self_attention,
                }
            )

            # ë‹¤ë¥¸ ìì‚°ë“¤ì— ëŒ€í•œ attention
            remaining_weight = 1.0 - self_attention
            other_weights = []

            for j, to_asset in enumerate(STOCK_SYMBOLS):
                if i == j or j >= data_stats.size(0):  # ìê¸° ìì‹ ì€ ì´ë¯¸ ì²˜ë¦¬
                    continue

                # ê¸°ë³¸ ìƒê´€ê´€ê³„ ê°€ì¤‘ì¹˜
                base_correlation = correlation_matrix.get(from_asset, {}).get(
                    to_asset, 0.1
                )

                # ë°ì´í„° ê¸°ë°˜ ì¡°ì •
                from_volatility = float(data_stats[i].std())
                to_volatility = float(data_stats[j].std())

                # ë³€ë™ì„±ì´ ë¹„ìŠ·í•œ ìì‚°ë“¤ ê°„ì˜ attention ì¦ê°€
                volatility_similarity = 1.0 - abs(from_volatility - to_volatility) / (
                    from_volatility + to_volatility + 1e-8
                )

                # ìµœì¢… ê°€ì¤‘ì¹˜ ê³„ì‚°
                final_weight = base_correlation * (0.7 + 0.3 * volatility_similarity)

                # ì•½ê°„ì˜ ëœë¤ì„± ì¶”ê°€
                final_weight *= 0.8 + 0.4 * np.random.random()

                other_weights.append((to_asset, final_weight))

            # ì •ê·œí™” (ë‚˜ë¨¸ì§€ ê°€ì¤‘ì¹˜ë“¤ì˜ í•©ì´ remaining_weightê°€ ë˜ë„ë¡)
            total_other = sum(w[1] for w in other_weights)
            if total_other > 0:
                for to_asset, weight in other_weights:
                    normalized_weight = (weight / total_other) * remaining_weight
                    attention_list.append(
                        {
                            "from_asset": from_asset,
                            "to_asset": to_asset,
                            "weight": normalized_weight,
                        }
                    )

        # ìƒìœ„ ê°€ì¤‘ì¹˜ ìˆœìœ¼ë¡œ ì •ë ¬
        attention_list.sort(key=lambda x: x["weight"], reverse=True)

        print(f"ì‹¤ìš©ì  Attention Weights ê³„ì‚° ì™„ë£Œ!")
        print(f"ìƒìœ„ 5ê°œ: {[round(f['weight'], 4) for f in attention_list[:5]]}")

        return attention_list[:50]  # ìƒìœ„ 50ê°œë§Œ ë°˜í™˜

    except Exception as e:
        print(f"Attention Weights ê³„ì‚° ì˜¤ë¥˜: {e}")
        import traceback

        traceback.print_exc()
        return []


def generate_explanation_text_with_method(
    feature_importance: List[Dict],
    attention_weights: List[Dict],
    allocation: List[Dict],
    method: str,
) -> str:
    """ê³„ì‚° ë°©ì‹ì„ ê³ ë ¤í•œ ì„¤ëª… í…ìŠ¤íŠ¸ ìƒì„±"""

    # ê¸°ë³¸ ì„¤ëª… ìƒì„±
    top_features = feature_importance[:5]
    top_assets = sorted(
        [a for a in allocation if a["symbol"] != "í˜„ê¸ˆ"],
        key=lambda x: x["weight"],
        reverse=True,
    )[:3]

    # ë°©ì‹ì— ë”°ë¥¸ í—¤ë”
    if method == "accurate":
        explanation = "ğŸ”¬ AI í¬íŠ¸í´ë¦¬ì˜¤ ê²°ì • ê·¼ê±° (ì •ë°€ ë¶„ì„):\n\n"
        explanation += "ğŸ“ˆ Perturbation ê¸°ë°˜ ì •í™•í•œ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤.\n\n"
    else:
        explanation = "âš¡ AI í¬íŠ¸í´ë¦¬ì˜¤ ê²°ì • ê·¼ê±° (ë¹ ë¥¸ ë¶„ì„):\n\n"
        explanation += "ğŸš€ ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ë¹ ë¥¸ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n\n"

    # ì£¼ìš” ì˜í–¥ ìš”ì¸
    explanation += "ğŸ” ì£¼ìš” ì˜í–¥ ìš”ì¸:\n"
    for i, feature in enumerate(top_features, 1):
        confidence = ""
        if method == "accurate":
            if feature["importance_score"] > 0.7:
                confidence = " (ë§¤ìš° ë†’ì€ ì‹ ë¢°ë„)"
            elif feature["importance_score"] > 0.4:
                confidence = " (ë†’ì€ ì‹ ë¢°ë„)"
            elif feature["importance_score"] > 0.2:
                confidence = " (ì¤‘ê°„ ì‹ ë¢°ë„)"
            else:
                confidence = " (ë‚®ì€ ì‹ ë¢°ë„)"

        explanation += f"{i}. {feature['asset_name']}ì˜ {feature['feature_name']}: {feature['importance_score']:.3f}{confidence}\n"

    explanation += "\nğŸ“Š í•µì‹¬ íˆ¬ì ë…¼ë¦¬:\n"

    # ìƒìœ„ ìì‚°ë³„ ì„¤ëª…
    for asset in top_assets:
        symbol = asset["symbol"]
        weight = asset["weight"] * 100

        # í•´ë‹¹ ìì‚°ì˜ ì£¼ìš” íŠ¹ì„± ì°¾ê¸°
        asset_features = [f for f in top_features if f["asset_name"] == symbol]

        if asset_features:
            main_feature = asset_features[0]["feature_name"]
            if method == "accurate":
                explanation += f"â€¢ {symbol} ({weight:.1f}%): {main_feature} ì§€í‘œê°€ ëª¨ë¸ ê²°ì •ì— ê°•í•œ ì˜í–¥\n"
            else:
                explanation += (
                    f"â€¢ {symbol} ({weight:.1f}%): {main_feature} ì§€í‘œê°€ ê¸ì •ì  ì‹ í˜¸\n"
                )
        else:
            explanation += f"â€¢ {symbol} ({weight:.1f}%): ì•ˆì •ì ì¸ ì„±ê³¼ ê¸°ëŒ€\n"

    # ìì‚° ê°„ ìƒê´€ê´€ê³„ ë¶„ì„ (attention weights í™œìš©)
    if attention_weights:
        explanation += "\nğŸ”— ìì‚° ê°„ ìƒê´€ê´€ê³„:\n"
        # ìê¸° ìì‹ ì„ ì œì™¸í•œ ìƒìœ„ attention weights ì°¾ê¸°
        cross_attention = [
            aw for aw in attention_weights if aw["from_asset"] != aw["to_asset"]
        ][:3]

        for aw in cross_attention:
            explanation += f"â€¢ {aw['from_asset']} â†’ {aw['to_asset']}: {aw['weight']:.3f} (ìƒí˜¸ ì˜í–¥ë„)\n"

    # ë¦¬ìŠ¤í¬ ê´€ë¦¬
    cash_allocation = next((a for a in allocation if a["symbol"] == "í˜„ê¸ˆ"), None)
    if cash_allocation and cash_allocation["weight"] > 0.1:
        explanation += f"\nğŸ›¡ï¸ ë¦¬ìŠ¤í¬ ê´€ë¦¬:\n"
        explanation += (
            f"â€¢ í˜„ê¸ˆ {cash_allocation['weight']*100:.1f}% ë³´ìœ ë¡œ ë³€ë™ì„± ì™„ì¶©\n"
        )
        if method == "accurate":
            explanation += f"â€¢ Perturbation ë¶„ì„ì„ í†µí•œ ì²´ê³„ì  ë¦¬ìŠ¤í¬ ê´€ë¦¬\n"

    # ë°©ì‹ë³„ ì¶”ê°€ ì •ë³´
    if method == "accurate":
        explanation += f"\nğŸ”¬ ë¶„ì„ ë°©ì‹: Perturbation ê¸°ë°˜ Feature Importance\n"
        explanation += f"â€¢ ê° íŠ¹ì„±ì„ ì‹¤ì œë¡œ ë³€í™”ì‹œì¼œ ëª¨ë¸ ë°˜ì‘ ì¸¡ì •\n"
        explanation += f"â€¢ KL Divergenceë¡œ ì˜ˆì¸¡ ë³€í™”ëŸ‰ ì •ëŸ‰í™”\n"
        explanation += f"â€¢ ë†’ì€ ì‹ ë¢°ë„ì™€ í•´ì„ ê°€ëŠ¥ì„± ë³´ì¥\n"
    else:
        explanation += f"\nâš¡ ë¶„ì„ ë°©ì‹: ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ íœ´ë¦¬ìŠ¤í‹±\n"
        explanation += f"â€¢ ê¸ˆìœµ ì „ë¬¸ê°€ ì§€ì‹ê³¼ ì‹œì¥ ë°ì´í„° ê²°í•©\n"
        explanation += f"â€¢ ë¹ ë¥¸ ì†ë„ë¡œ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ì œê³µ\n"
        explanation += f"â€¢ ì‹¤ì‹œê°„ ì˜ì‚¬ê²°ì • ì§€ì›ì— ìµœì í™”\n"

    return explanation


@app.post("/explain", response_model=XAIResponse)
async def explain_prediction(request: XAIRequest):
    """XAI ì„¤ëª… ì—”ë“œí¬ì¸íŠ¸ (ê³„ì‚° ë°©ì‹ ì„ íƒ ê°€ëŠ¥)"""

    if model is None:
        raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    try:
        import asyncio
        import time

        method = request.method.lower()
        print(f"XAI ë¶„ì„ ì‹œì‘: íˆ¬ìê¸ˆì•¡={request.investment_amount}, ë°©ì‹={method}")

        # ì‹œì‘ ì‹œê°„ ê¸°ë¡
        start_time = time.time()

        # ì‹œì¥ ë°ì´í„° ì¤€ë¹„
        market_data = get_market_data_with_context(
            request.investment_amount, request.risk_tolerance
        )

        if market_data is None:
            raise HTTPException(
                status_code=500, detail="ì‹œì¥ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )

        enhanced_data = enhance_data_with_user_context(
            market_data,
            request.investment_amount,
            request.risk_tolerance,
            request.investment_horizon,
        )

        input_tensor = torch.FloatTensor(enhanced_data).unsqueeze(0).to(DEVICE)

        # ê³„ì‚° ë°©ì‹ì— ë”°ë¥¸ Feature Importance ê³„ì‚°
        if method == "accurate":
            print("ì •í™•í•œ ë¶„ì„ ê³„ì‚° ì‹œì‘ (ì˜ˆìƒ 30ì´ˆ-2ë¶„)")
            feature_importance = calculate_feature_importance(model, input_tensor)

            # ë§Œì•½ ê²°ê³¼ê°€ ëª¨ë‘ 0ì´ë©´ ë¹ ë¥¸ ë°©ë²•ìœ¼ë¡œ í´ë°±
            if all(f["importance_score"] == 0.0 for f in feature_importance):
                print("ì •í™•í•œ ë¶„ì„ ê²°ê³¼ê°€ ëª¨ë‘ 0 - ë¹ ë¥¸ ë°©ë²•ìœ¼ë¡œ í´ë°±")
                feature_importance = calculate_feature_importance_fast(
                    model, input_tensor
                )
        else:  # "fast"
            print("ë¹ ë¥¸ ë¶„ì„ ê³„ì‚° ì‹œì‘ (ì˜ˆìƒ 5-10ì´ˆ)")
            feature_importance = calculate_feature_importance_fast(model, input_tensor)

        # Attention weights ê³„ì‚°
        print("ìì‚° ê°„ ìƒê´€ê´€ê³„ ë¶„ì„ ì¤‘...")
        attention_weights = extract_attention_weights(model, input_tensor)

        # ì˜ˆì¸¡ ê²°ê³¼ ê³„ì‚°
        prediction_result = predict_portfolio(
            request.investment_amount,
            request.risk_tolerance,
            request.investment_horizon,
        )

        # ê³„ì‚° ë°©ì‹ì— ë”°ë¥¸ ì„¤ëª… í…ìŠ¤íŠ¸ ìƒì„±
        explanation_text = generate_explanation_text_with_method(
            feature_importance,
            attention_weights,
            prediction_result["allocation"],
            method,
        )

        # ê²½ê³¼ ì‹œê°„ ê³„ì‚° ë° ìµœì†Œ ëŒ€ê¸° ì‹œê°„ í™•ë³´
        elapsed_time = time.time() - start_time
        min_duration = 3.0 if method == "fast" else 8.0  # ìµœì†Œ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)

        if elapsed_time < min_duration:
            remaining_time = min_duration - elapsed_time
            print(f"ì‚¬ìš©ì ê²½í—˜ í–¥ìƒì„ ìœ„í•´ {remaining_time:.1f}ì´ˆ ì¶”ê°€ ëŒ€ê¸°")
            await asyncio.sleep(remaining_time)

        print(
            f"XAI ë¶„ì„ ì™„ë£Œ! (ë°©ì‹: {method}, ì´ ì†Œìš”ì‹œê°„: {time.time() - start_time:.1f}ì´ˆ)"
        )

        return XAIResponse(
            feature_importance=[
                FeatureImportance(
                    feature_name=item["feature_name"],
                    importance_score=item["importance_score"],
                    asset_name=item["asset_name"],
                )
                for item in feature_importance
            ],
            attention_weights=[
                AttentionWeight(
                    from_asset=item["from_asset"],
                    to_asset=item["to_asset"],
                    weight=item["weight"],
                )
                for item in attention_weights
            ],
            explanation_text=explanation_text,
        )

    except Exception as e:
        print(f"XAI ì„¤ëª… ìƒì„± ì˜¤ë¥˜: {e}")
        import traceback

        traceback.print_exc()
        raise HTTPException(
            status_code=500, detail="XAI ì„¤ëª… ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        )


# ìƒˆë¡œìš´ API ì—”ë“œí¬ì¸íŠ¸ë“¤
def calculate_historical_performance(
    allocation: List[AllocationItem],
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
) -> List[PerformanceHistory]:
    """ì‹¤ì œ ë°±í…ŒìŠ¤íŠ¸ ê¸°ë°˜ ì„±ê³¼ íˆìŠ¤í† ë¦¬ ê³„ì‚°"""
    try:
        # ë‚ ì§œ ì„¤ì •
        if end_date is None:
            end_date = datetime.now().strftime("%Y-%m-%d")
        if start_date is None:
            start_date = (datetime.now() - timedelta(days=365)).strftime("%Y-%m-%d")

        # í¬íŠ¸í´ë¦¬ì˜¤ ì¢…ëª©ë“¤ ì¶”ì¶œ
        portfolio_tickers = [
            item.symbol for item in allocation if item.symbol != "í˜„ê¸ˆ"
        ]
        if not portfolio_tickers:
            # í˜„ê¸ˆë§Œ ìˆëŠ” ê²½ìš° ê¸°ë³¸ ìˆ˜ìµë¥  0
            return []

        # ë²¤ì¹˜ë§ˆí¬ ì¶”ê°€
        all_tickers = portfolio_tickers + ["SPY", "QQQ"]

        # ì‹¤ì œ ê°€ê²© ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ì„¸ì…˜ ì‚¬ìš©)
        print(f"ë‹¤ìš´ë¡œë“œ ì¤‘: {all_tickers}, ê¸°ê°„: {start_date} ~ {end_date}")

        # curl_cffi ì„¸ì…˜ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ ë°©ì‹
        if session is not None:
            # ê° í‹°ì»¤ë³„ë¡œ ê°œë³„ ë‹¤ìš´ë¡œë“œ (Rate Limit íšŒí”¼)
            data_dict = {}
            for ticker in all_tickers:
                try:
                    ticker_obj = yf.Ticker(ticker, session=session)
                    ticker_data = ticker_obj.history(start=start_date, end=end_date)
                    if not ticker_data.empty:
                        data_dict[ticker] = ticker_data
                        print(f"âœ“ {ticker} ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì„±ê³µ")
                    else:
                        print(f"âœ— {ticker} ë°ì´í„° ì—†ìŒ")
                except Exception as e:
                    print(f"âœ— {ticker} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")

            # ë°ì´í„° ì¡°í•©
            if data_dict:
                # ëª¨ë“  í‹°ì»¤ì˜ Close ê°€ê²©ì„ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ ì¡°í•©
                close_prices = pd.DataFrame()
                for ticker, ticker_data in data_dict.items():
                    close_prices[ticker] = ticker_data["Close"]
                close_prices = close_prices.dropna()
            else:
                print("ëª¨ë“  í‹°ì»¤ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
                return []
        else:
            # ê¸°ë³¸ ë°©ì‹
            data = yf.download(
                all_tickers, start=start_date, end=end_date, progress=False
            )
            if data.empty:
                print("ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ")
                return []

            # ì¢…ê°€ ë°ì´í„° ì¶”ì¶œ
            if len(all_tickers) == 1:
                close_prices = data["Close"].to_frame()
                close_prices.columns = all_tickers
            else:
                close_prices = data["Close"]

        # ì¼ì¼ ìˆ˜ìµë¥  ê³„ì‚°
        daily_returns = close_prices.pct_change().dropna()

        # í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¤‘ì¹˜ ì ìš©
        portfolio_weights = {
            item.symbol: item.weight for item in allocation if item.symbol != "í˜„ê¸ˆ"
        }

        # í¬íŠ¸í´ë¦¬ì˜¤ ì¼ì¼ ìˆ˜ìµë¥  ê³„ì‚°
        portfolio_daily_returns = []
        for date in daily_returns.index:
            daily_return = 0.0
            for ticker in portfolio_tickers:
                if ticker in daily_returns.columns and ticker in portfolio_weights:
                    daily_return += (
                        daily_returns.loc[date, ticker] * portfolio_weights[ticker]
                    )
            portfolio_daily_returns.append(daily_return)

        # ëˆ„ì  ìˆ˜ìµë¥  ê³„ì‚°
        portfolio_cumulative = np.cumprod(1 + np.array(portfolio_daily_returns)) - 1
        spy_cumulative = (
            np.cumprod(1 + daily_returns["SPY"].values) - 1
            if "SPY" in daily_returns.columns
            else np.zeros(len(portfolio_cumulative))
        )
        qqq_cumulative = (
            np.cumprod(1 + daily_returns["QQQ"].values) - 1
            if "QQQ" in daily_returns.columns
            else np.zeros(len(portfolio_cumulative))
        )

        # ê²°ê³¼ ìƒì„±
        performance_history = []
        for i, date in enumerate(daily_returns.index):
            performance_history.append(
                PerformanceHistory(
                    date=date.strftime("%Y-%m-%d"),
                    portfolio=float(portfolio_cumulative[i]),
                    spy=float(spy_cumulative[i]),
                    qqq=float(qqq_cumulative[i]),
                )
            )

        return performance_history

    except Exception as e:
        print(f"ë°±í…ŒìŠ¤íŠ¸ ê³„ì‚° ì˜¤ë¥˜: {e}")
        return []


def calculate_real_correlation(
    tickers: List[str], period: str = "1y"
) -> List[CorrelationData]:
    """ì‹¤ì œ ì¢…ëª© ê°„ ìƒê´€ê´€ê³„ ê³„ì‚°"""
    try:
        # í˜„ê¸ˆ ì œì™¸
        stock_tickers = [ticker for ticker in tickers if ticker != "í˜„ê¸ˆ"]
        if len(stock_tickers) < 2:
            return []

        # ê°€ê²© ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ì„¸ì…˜ ì‚¬ìš©)
        print(f"ìƒê´€ê´€ê³„ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ: {stock_tickers}, ê¸°ê°„: {period}")

        # curl_cffi ì„¸ì…˜ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ ë°©ì‹
        if session is not None:
            # ê° í‹°ì»¤ë³„ë¡œ ê°œë³„ ë‹¤ìš´ë¡œë“œ (Rate Limit íšŒí”¼)
            data_dict = {}
            for ticker in stock_tickers:
                try:
                    ticker_obj = yf.Ticker(ticker, session=session)
                    ticker_data = ticker_obj.history(period=period)
                    if not ticker_data.empty:
                        data_dict[ticker] = ticker_data
                        print(f"âœ“ {ticker} ìƒê´€ê´€ê³„ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì„±ê³µ")
                    else:
                        print(f"âœ— {ticker} ìƒê´€ê´€ê³„ ë°ì´í„° ì—†ìŒ")
                except Exception as e:
                    print(f"âœ— {ticker} ìƒê´€ê´€ê³„ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")

            # ë°ì´í„° ì¡°í•©
            if len(data_dict) >= 2:
                # ëª¨ë“  í‹°ì»¤ì˜ Close ê°€ê²©ì„ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ ì¡°í•©
                close_prices = pd.DataFrame()
                for ticker, ticker_data in data_dict.items():
                    close_prices[ticker] = ticker_data["Close"]
                close_prices = close_prices.dropna()
            else:
                print("ìƒê´€ê´€ê³„ ë¶„ì„ì„ ìœ„í•œ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŒ")
                return []
        else:
            # ê¸°ë³¸ ë°©ì‹
            data = yf.download(stock_tickers, period=period, progress=False)
            if data.empty:
                return []

            # ì¢…ê°€ ë°ì´í„° ì¶”ì¶œ
            if len(stock_tickers) == 1:
                close_prices = data["Close"].to_frame()
                close_prices.columns = stock_tickers
            else:
                close_prices = data["Close"]

        # ì¼ì¼ ìˆ˜ìµë¥  ê³„ì‚°
        daily_returns = close_prices.pct_change().dropna()

        # ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
        correlation_matrix = daily_returns.corr()

        # ìƒê´€ê´€ê³„ ë°ì´í„° ìƒì„±
        correlation_data = []
        available_tickers = list(correlation_matrix.columns)
        for i, stock1 in enumerate(available_tickers):
            for j, stock2 in enumerate(available_tickers):
                if i < j:  # ì¤‘ë³µ ì œê±°
                    correlation = correlation_matrix.loc[stock1, stock2]
                    if not np.isnan(correlation):
                        correlation_data.append(
                            CorrelationData(
                                stock1=stock1,
                                stock2=stock2,
                                correlation=float(correlation),
                            )
                        )

        return correlation_data

    except Exception as e:
        print(f"ìƒê´€ê´€ê³„ ê³„ì‚° ì˜¤ë¥˜: {e}")
        return []


def calculate_risk_return_data(
    allocation: List[AllocationItem], period: str = "1y"
) -> List[RiskReturnData]:
    """ì‹¤ì œ ë¦¬ìŠ¤í¬-ìˆ˜ìµë¥  ë°ì´í„° ê³„ì‚°"""
    try:
        # í¬íŠ¸í´ë¦¬ì˜¤ ì¢…ëª©ë“¤ ì¶”ì¶œ
        portfolio_tickers = [
            item.symbol for item in allocation if item.symbol != "í˜„ê¸ˆ"
        ]
        if not portfolio_tickers:
            return []

        # ê°€ê²© ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ì„¸ì…˜ ì‚¬ìš©)
        print(
            f"ë¦¬ìŠ¤í¬-ìˆ˜ìµë¥  ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ: {portfolio_tickers}, ê¸°ê°„: {period}"
        )

        # curl_cffi ì„¸ì…˜ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ ë°©ì‹
        if session is not None:
            # ê° í‹°ì»¤ë³„ë¡œ ê°œë³„ ë‹¤ìš´ë¡œë“œ (Rate Limit íšŒí”¼)
            data_dict = {}
            for ticker in portfolio_tickers:
                try:
                    ticker_obj = yf.Ticker(ticker, session=session)
                    ticker_data = ticker_obj.history(period=period)
                    if not ticker_data.empty:
                        data_dict[ticker] = ticker_data
                        print(f"âœ“ {ticker} ë¦¬ìŠ¤í¬-ìˆ˜ìµë¥  ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì„±ê³µ")
                    else:
                        print(f"âœ— {ticker} ë¦¬ìŠ¤í¬-ìˆ˜ìµë¥  ë°ì´í„° ì—†ìŒ")
                except Exception as e:
                    print(f"âœ— {ticker} ë¦¬ìŠ¤í¬-ìˆ˜ìµë¥  ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")

            # ë°ì´í„° ì¡°í•©
            if data_dict:
                # ëª¨ë“  í‹°ì»¤ì˜ Close ê°€ê²©ì„ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ ì¡°í•©
                close_prices = pd.DataFrame()
                for ticker, ticker_data in data_dict.items():
                    close_prices[ticker] = ticker_data["Close"]
                close_prices = close_prices.dropna()
            else:
                print("ë¦¬ìŠ¤í¬-ìˆ˜ìµë¥  ë¶„ì„ì„ ìœ„í•œ ë°ì´í„°ê°€ ì—†ìŒ")
                return []
        else:
            # ê¸°ë³¸ ë°©ì‹
            data = yf.download(portfolio_tickers, period=period, progress=False)
            if data.empty:
                return []

            # ì¢…ê°€ ë°ì´í„° ì¶”ì¶œ
            if len(portfolio_tickers) == 1:
                close_prices = data["Close"].to_frame()
                close_prices.columns = portfolio_tickers
            else:
                close_prices = data["Close"]

        # ì¼ì¼ ìˆ˜ìµë¥  ê³„ì‚°
        daily_returns = close_prices.pct_change().dropna()

        # ê° ì¢…ëª©ë³„ ë¦¬ìŠ¤í¬-ìˆ˜ìµë¥  ê³„ì‚°
        risk_return_data = []
        for item in allocation:
            if item.symbol == "í˜„ê¸ˆ":
                continue

            if item.symbol in daily_returns.columns:
                returns = daily_returns[item.symbol]

                # ì—°ê°„ ìˆ˜ìµë¥  ê³„ì‚° (252 ê±°ë˜ì¼ ê¸°ì¤€)
                annual_return = returns.mean() * 252 * 100

                # ì—°ê°„ ë³€ë™ì„± ê³„ì‚° (í‘œì¤€í¸ì°¨)
                annual_volatility = returns.std() * np.sqrt(252) * 100

                risk_return_data.append(
                    RiskReturnData(
                        symbol=item.symbol,
                        risk=float(annual_volatility),
                        return_rate=float(annual_return),
                        allocation=float(item.weight * 100),
                    )
                )

        return risk_return_data

    except Exception as e:
        print(f"ë¦¬ìŠ¤í¬-ìˆ˜ìµë¥  ê³„ì‚° ì˜¤ë¥˜: {e}")
        return []


def get_real_market_data() -> MarketStatusResponse:
    """ì‹¤ì‹œê°„ ì‹œì¥ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
    try:
        # ì£¼ìš” ì‹œì¥ ì§€ìˆ˜ë“¤
        market_symbols = {
            "^GSPC": "S&P 500",
            "^IXIC": "NASDAQ",
            "^VIX": "VIX ë³€ë™ì„± ì§€ìˆ˜",
            "KRW=X": "USD/KRW í™˜ìœ¨",
        }

        market_data = []
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        for symbol, name in market_symbols.items():
            try:
                # curl_cffi ì„¸ì…˜ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ ë°©ì‹
                if session is not None:
                    ticker = yf.Ticker(symbol, session=session)
                    print(f"âœ“ {name} ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹œë„ (ì„¸ì…˜ ì‚¬ìš©)")
                else:
                    ticker = yf.Ticker(symbol)
                    print(f"âœ“ {name} ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹œë„ (ê¸°ë³¸ ë°©ì‹)")

                hist = ticker.history(period="2d")  # ìµœê·¼ 2ì¼ ë°ì´í„°

                if not hist.empty:
                    current_price = hist["Close"].iloc[-1]
                    previous_price = (
                        hist["Close"].iloc[-2] if len(hist) > 1 else current_price
                    )

                    change = current_price - previous_price
                    change_percent = (
                        (change / previous_price) * 100 if previous_price != 0 else 0
                    )

                    market_data.append(
                        MarketData(
                            symbol=symbol,
                            name=name,
                            price=float(current_price),
                            change=float(change),
                            change_percent=float(change_percent),
                            last_updated=current_time,
                        )
                    )
                    print(f"âœ“ {name} ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì„±ê³µ")
                else:
                    print(f"âœ— {name} íˆìŠ¤í† ë¦¬ ë°ì´í„° ì—†ìŒ")

            except Exception as e:
                print(f"âœ— {symbol} ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨í•œ ê²½ìš° ê¸°ë³¸ê°’ ì¶”ê°€
                market_data.append(
                    MarketData(
                        symbol=symbol,
                        name=name,
                        price=0.0,
                        change=0.0,
                        change_percent=0.0,
                        last_updated=current_time,
                    )
                )

        return MarketStatusResponse(market_data=market_data, last_updated=current_time)

    except Exception as e:
        print(f"ì‹œì¥ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
        return MarketStatusResponse(
            market_data=[], last_updated=datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        )


@app.post("/historical-performance", response_model=HistoricalResponse)
async def get_historical_performance(request: HistoricalRequest):
    """ì‹¤ì œ ë°±í…ŒìŠ¤íŠ¸ ê¸°ë°˜ ì„±ê³¼ íˆìŠ¤í† ë¦¬"""
    try:
        performance_history = calculate_historical_performance(
            request.portfolio_allocation, request.start_date, request.end_date
        )

        return HistoricalResponse(performance_history=performance_history)

    except Exception as e:
        print(f"ì„±ê³¼ íˆìŠ¤í† ë¦¬ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(
            status_code=500, detail="ì„±ê³¼ íˆìŠ¤í† ë¦¬ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        )


@app.post("/correlation-analysis", response_model=CorrelationResponse)
async def get_correlation_analysis(request: CorrelationRequest):
    """ì‹¤ì œ ì¢…ëª© ê°„ ìƒê´€ê´€ê³„ ë¶„ì„"""
    try:
        correlation_data = calculate_real_correlation(request.tickers, request.period)

        return CorrelationResponse(correlation_data=correlation_data)

    except Exception as e:
        print(f"ìƒê´€ê´€ê³„ ë¶„ì„ ì˜¤ë¥˜: {e}")
        raise HTTPException(
            status_code=500, detail="ìƒê´€ê´€ê³„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        )


@app.post("/risk-return-analysis", response_model=RiskReturnResponse)
async def get_risk_return_analysis(request: RiskReturnRequest):
    """ì‹¤ì œ ë¦¬ìŠ¤í¬-ìˆ˜ìµë¥  ë¶„ì„"""
    try:
        risk_return_data = calculate_risk_return_data(
            request.portfolio_allocation, request.period
        )

        return RiskReturnResponse(risk_return_data=risk_return_data)

    except Exception as e:
        print(f"ë¦¬ìŠ¤í¬-ìˆ˜ìµë¥  ë¶„ì„ ì˜¤ë¥˜: {e}")
        raise HTTPException(
            status_code=500, detail="ë¦¬ìŠ¤í¬-ìˆ˜ìµë¥  ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        )


@app.get("/market-status", response_model=MarketStatusResponse)
async def get_market_status():
    """ì‹¤ì‹œê°„ ì‹œì¥ ë°ì´í„°"""
    try:
        return get_real_market_data()

    except Exception as e:
        print(f"ì‹œì¥ ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(
            status_code=500, detail="ì‹œì¥ ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        )


def download_with_retry(ticker: str, **kwargs) -> pd.DataFrame:
    """ì¬ì‹œë„ ë¡œì§ì´ ìˆëŠ” yfinance ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜"""
    max_retries = 3
    base_delay = 1.0

    for attempt in range(max_retries):
        try:
            # í‹°ì»¤ ê°ì²´ ìƒì„± ì‹œ ì„¸ì…˜ ì‚¬ìš©
            if session is not None:
                ticker_obj = yf.Ticker(ticker, session=session)
            else:
                ticker_obj = yf.Ticker(ticker)

            # ë°ì´í„° ë‹¤ìš´ë¡œë“œ
            data = ticker_obj.history(**kwargs)

            if not data.empty:
                print(f"âœ“ {ticker} ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì„±ê³µ (ì‹œë„ {attempt + 1})")
                return data
            else:
                print(f"âœ— {ticker} ë°ì´í„° ì—†ìŒ (ì‹œë„ {attempt + 1})")

        except Exception as e:
            print(f"âœ— {ticker} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}): {e}")

        # ì¬ì‹œë„ ì „ ëŒ€ê¸° (ì§€ìˆ˜ ë°±ì˜¤í”„)
        if attempt < max_retries - 1:
            delay = base_delay * (2**attempt) + random.uniform(0, 1)
            print(f"  {delay:.1f}ì´ˆ í›„ ì¬ì‹œë„...")
            time.sleep(delay)

    print(f"âœ— {ticker} ëª¨ë“  ì‹œë„ ì‹¤íŒ¨")
    return pd.DataFrame()


def download_multiple_tickers(tickers: List[str], **kwargs) -> Dict[str, pd.DataFrame]:
    """ì—¬ëŸ¬ í‹°ì»¤ë¥¼ ê°œë³„ì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ"""
    results = {}

    for ticker in tickers:
        # ìš”ì²­ ê°„ ê°„ê²© ì¶”ê°€ (Rate Limiting ë°©ì§€)
        if len(results) > 0:
            time.sleep(0.5)

        data = download_with_retry(ticker, **kwargs)
        if not data.empty:
            results[ticker] = data

    return results


if __name__ == "__main__":
    uvicorn.run(
        "rl_inference_server:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info",
    )
